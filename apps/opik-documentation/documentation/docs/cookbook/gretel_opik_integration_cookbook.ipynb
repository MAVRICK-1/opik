{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a77a7fb6",
   "metadata": {},
   "source": [
    "# Gretel AI to Opik Dataset Integration - Complete Cookbook\n",
    "\n",
    "A comprehensive guide with ready-to-run examples for generating synthetic Q&A datasets using Gretel Navigator and importing them into Opik for model evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 What This Cookbook Covers\n",
    "\n",
    "- **Authentication setup** for both Gretel and Opik\n",
    "- **Synthetic data generation** using Gretel Navigator\n",
    "- **Data format conversion** from Gretel to Opik\n",
    "- **Dataset import** into Opik for evaluation\n",
    "- **Complete examples** for different use cases\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Prerequisites & Setup\n",
    "\n",
    "Before starting, you'll need:\n",
    "1. **Gretel Account**: Sign up at [gretel.ai](https://gretel.ai)\n",
    "2. **Comet Account**: Sign up at [comet.com](https://comet.com) for Opik access\n",
    "3. **API Keys**: Gretel API key and Comet API key\n",
    "\n",
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fab09f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gretel_client opik langchain tiktoken pandas --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81c8e480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Opik is already configured. You can check the settings by viewing the config file at /home/mavrick/.opik.config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Gretel to Opik integration setup...\n",
      "Logged in as mavrickrishi@gmail.com ✅\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from gretel_client import Gretel\n",
    "import opik\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "print(\"🚀 Starting Gretel to Opik integration setup...\")\n",
    "\n",
    "#set up Opik\n",
    "opik.configure()\n",
    "\n",
    "# Set up Gretel API key\n",
    "if \"GRETEL_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GRETEL_API_KEY\"] = getpass.getpass(\"Enter your Gretel API key: \")\n",
    "\n",
    "gretel = Gretel(api_key=os.environ[\"GRETEL_API_KEY\"], cache=True, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd657be",
   "metadata": {},
   "source": [
    "## Find Working Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29b3647b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Validating setup...\n",
      "🔍 Finding working Navigator model...\n",
      "🔍 Testing 4 available models...\n",
      "🔄 Testing model 1/4: gretelai/auto\n",
      "Backend model: gretelai/auto\n",
      "API path: https://api.gretel.cloud/v1/inference/tabular/\n",
      "Navigator Tabular initialized 🚀\n",
      "Backend model: gretelai/auto\n",
      "Navigator Tabular initialized 🚀\n",
      "🧪 Running test generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating records:   0%|          | 0/3 [00:00,? records/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received error during generation: 'Unfortunately the AI system could not return data. Please make sure that your request is valid and relevant for tabular data, and does not include sensitive topics. Clearly writing out your request with examples or bulleted list of columns can help. (error_code: MalformedResponseError) Please try again or contact support.'. Retrying.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating records: 100%|██████████| 3/3 [00:20, 0.15 records/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model gretelai/auto works! Generated 3 records\n",
      "📊 Test database:\n",
      "            name  age\n",
      "0    Aisha Patel   25\n",
      "1      Liam Chen   30\n",
      "2  Nalani Jensen   28\n",
      "📋 Columns: ['name', 'age']\n",
      "📈 Data types: {'name': dtype('O'), 'age': dtype('int64')}\n",
      "\n",
      "🎉 Setup completed successfully!\n",
      "✅ Using working model: gretelai/auto\n",
      "📊 Navigator type: TabularInferenceAPI\n",
      "✅ Navigator and working_model are now available globally\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def find_working_navigator_model():\n",
    "    \"\"\"Find a Navigator model that works in the current environment with enhanced error handling\"\"\"\n",
    "    models_to_try = [\n",
    "        'gretelai/auto', \n",
    "        'gretelai/apache-2.0', \n",
    "        'gretelai/llama-3.x',\n",
    "        'gretelai/gpt-3.5-turbo'\n",
    "    ]\n",
    "    \n",
    "    print(f\"🔍 Testing {len(models_to_try)} available models...\")\n",
    "    \n",
    "    for i, model in enumerate(models_to_try, 1):\n",
    "        try:\n",
    "            print(f\"🔄 Testing model {i}/{len(models_to_try)}: {model}\")\n",
    "            \n",
    "            # Initialize navigator with timeout handling\n",
    "            test_navigator = gretel.factories.initialize_navigator_api(\n",
    "                \"tabular\", \n",
    "                backend_model=model\n",
    "            )\n",
    "            \n",
    "            print(f\"Backend model: {model}\")\n",
    "            print(\"Navigator Tabular initialized 🚀\")\n",
    "            \n",
    "            # More comprehensive test with timeout\n",
    "            print(\"🧪 Running test generation...\")\n",
    "            test_result = test_navigator.generate(\n",
    "                \"Create a small dataset with columns 'name' and 'age' for 3 people.\", \n",
    "                num_records=3\n",
    "            )\n",
    "            \n",
    "            # Validate result structure\n",
    "            if test_result is not None and len(test_result) > 0:\n",
    "                # Check if expected columns exist\n",
    "                if 'name' in test_result.columns and 'age' in test_result.columns:\n",
    "                    print(f\"✅ Model {model} works! Generated {len(test_result)} records\")\n",
    "                    print(f\"📊 Test database:\")\n",
    "                    print(test_result)\n",
    "                    print(f\"📋 Columns: {list(test_result.columns)}\")\n",
    "                    print(f\"📈 Data types: {dict(test_result.dtypes)}\")\n",
    "                    return test_navigator, model\n",
    "                else:\n",
    "                    print(f\"⚠️ Model {model} returned data but with unexpected columns: {list(test_result.columns)}\")\n",
    "                    # Still might work for our use case, continue testing\n",
    "                    if len(test_result.columns) >= 2:  # At least some structured data\n",
    "                        print(f\"✅ Model {model} works with alternative structure! Generated {len(test_result)} records\")\n",
    "                        print(f\"📊 Test database:\")\n",
    "                        print(test_result)\n",
    "                        return test_navigator, model\n",
    "            else:\n",
    "                print(f\"⚠️ Model {model} returned empty or invalid data\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"❌ Model {model} failed: {error_msg[:100]}{'...' if len(error_msg) > 100 else ''}\")\n",
    "            \n",
    "            # Check for specific error types\n",
    "            if \"authentication\" in error_msg.lower():\n",
    "                print(\"🔐 Authentication issue detected - check your API key\")\n",
    "            elif \"timeout\" in error_msg.lower():\n",
    "                print(\"⏱️ Timeout issue - network or service may be slow\")\n",
    "            elif \"quota\" in error_msg.lower() or \"limit\" in error_msg.lower():\n",
    "                print(\"📊 Rate limit or quota issue detected\")\n",
    "    \n",
    "    # If we get here, no model worked\n",
    "    print(\"\\n❌ No working Navigator model found!\")\n",
    "    print(\"🔧 Troubleshooting suggestions:\")\n",
    "    print(\"   1. Check your Gretel API key is valid\")\n",
    "    print(\"   2. Verify your internet connection\")\n",
    "    print(\"   3. Check if your account has sufficient credits\")\n",
    "    print(\"   4. Try again in a few minutes (rate limiting)\")\n",
    "    \n",
    "    raise Exception(\"No working Navigator model found. Check your API key and network connection.\")\n",
    "\n",
    "def validate_navigator_setup():\n",
    "    \"\"\"Validate that navigator is properly set up\"\"\"\n",
    "    required_vars = ['gretel']\n",
    "    missing_vars = []\n",
    "    \n",
    "    for var_name in required_vars:\n",
    "        if var_name not in globals() or globals()[var_name] is None:\n",
    "            missing_vars.append(var_name)\n",
    "    \n",
    "    if missing_vars:\n",
    "        raise NameError(f\"Required variables not found: {missing_vars}. Please run setup cells first.\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Main execution\n",
    "try:\n",
    "    print(\"🔍 Validating setup...\")\n",
    "    validate_navigator_setup()\n",
    "    \n",
    "    print(\"🔍 Finding working Navigator model...\")\n",
    "    navigator, working_model = find_working_navigator_model()\n",
    "    \n",
    "    print(f\"\\n🎉 Setup completed successfully!\")\n",
    "    print(f\"✅ Using working model: {working_model}\")\n",
    "    print(f\"📊 Navigator type: {type(navigator).__name__}\")\n",
    "    \n",
    "    # Verify navigator is available globally\n",
    "    if navigator is None:\n",
    "        raise Exception(\"Navigator initialization failed - navigator is None\")\n",
    "    \n",
    "    # Store in global scope for other cells\n",
    "    globals()['navigator'] = navigator\n",
    "    globals()['working_model'] = working_model\n",
    "    \n",
    "    print(\"✅ Navigator and working_model are now available globally\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Setup failed: {e}\")\n",
    "    print(\"\\n🔧 Please check:\")\n",
    "    print(\"   1. Run Cell 1 (install packages) first\")\n",
    "    print(\"   2. Run Cell 2 (authentication setup) first\") \n",
    "    print(\"   3. Check your Gretel API key\")\n",
    "    print(\"   4. Verify internet connection\")\n",
    "    \n",
    "    # Set fallback values to prevent downstream errors\n",
    "    navigator = None\n",
    "    working_model = None\n",
    "    \n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96757b6",
   "metadata": {},
   "source": [
    "## 📝 Configure Prompt and Source Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49d88b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Prompt and source text configured\n",
      "📄 Source text length: 596 characters\n"
     ]
    }
   ],
   "source": [
    "# Base prompt for Q&A dataset creation\n",
    "PROMPT = (\n",
    "    \"From the source text below, create a dataset with the following columns:\\n\"\n",
    "    \"* `question`: Ask a set of unique questions related to the topic that a customer might ask. \"\n",
    "    \"Questions should be relatively complex and specific enough to be addressed in a short answer.\\n\"\n",
    "    \"* `context`: Copy the exact sentence(s) from the source text and surrounding details from where the answer can be derived.\\n\"\n",
    "    \"* `truth`: Respond to the question with a clear, textbook quality answer that provides relevant details to fully address the question.\\n\"\n",
    ")\n",
    "\n",
    "# Your source content (customize this with your domain-specific content)\n",
    "source_text = \"\"\"\n",
    "Artificial Intelligence (AI) has revolutionized numerous industries by automating complex tasks \n",
    "and providing intelligent insights. Machine learning, a subset of AI, enables systems to learn \n",
    "from data without explicit programming. Deep learning, using neural networks with multiple layers, \n",
    "has achieved breakthroughs in image recognition, natural language processing, and decision making.\n",
    "The field continues to evolve with advancements in transformer architectures, reinforcement learning,\n",
    "and federated learning approaches that preserve privacy while enabling collaborative model training.\n",
    "\"\"\"\n",
    "\n",
    "print(\"📝 Prompt and source text configured\")\n",
    "print(f\"📄 Source text length: {len(source_text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66dd1c9",
   "metadata": {},
   "source": [
    "## 🚀 Generate Synthetic Q&A Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e793e9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting data generation...\n",
      "🔄 Attempt 1: Generating with params {'num_records': 10, 'temperature': 0.7, 'top_p': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating records: 100%|██████████| 10/10 [00:28, 0.35 records/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success! Generated 10 records\n",
      "\n",
      "📊 Dataset generated successfully!\n",
      "   Shape: (10, 3)\n",
      "   Columns: ['question', 'context', 'truth']\n",
      "\n",
      "📋 Sample generated data:\n",
      "                                                  question  \\\n",
      "0  What is the primary function of machine learning in AI?   \n",
      "1         What are some key applications of deep learning?   \n",
      "2               What is the purpose of federated learning?   \n",
      "\n",
      "                                                                                               context  \\\n",
      "0   Machine learning, a subset of AI, enables systems to learn from data without explicit programming.   \n",
      "1  Deep learning, using neural networks with multiple layers, has achieved breakthroughs in image r...   \n",
      "2  The field continues to evolve with advancements in transformer architectures, reinforcement lear...   \n",
      "\n",
      "                                                                                                 truth  \n",
      "0  Machine learning allows systems to learn from data without explicit programming, enabling them t...  \n",
      "1  Deep learning has achieved breakthroughs in image recognition, natural language processing, and ...  \n",
      "2  Federated learning is an approach that preserves privacy while enabling collaborative model trai...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_qa_dataset_robust(navigator, prompt, source_text, max_attempts=3):\n",
    "    \"\"\"Generate Q&A dataset with multiple fallback strategies\"\"\"\n",
    "    \n",
    "    # Different parameter strategies (from complex to simple)\n",
    "    strategies = [\n",
    "        {\"num_records\": 10, \"temperature\": 0.7, \"top_p\": 0.9},\n",
    "        {\"num_records\": 8, \"temperature\": 0.5},\n",
    "        {\"num_records\": 5},\n",
    "        {\"num_records\": 3, \"temperature\": 0.3}\n",
    "    ]\n",
    "    \n",
    "    for attempt, params in enumerate(strategies, 1):\n",
    "        try:\n",
    "            print(f\"🔄 Attempt {attempt}: Generating with params {params}\")\n",
    "            result = navigator.generate(f\"{prompt}\\n\\n{source_text}\", **params)\n",
    "            \n",
    "            if len(result) > 0:\n",
    "                print(f\"✅ Success! Generated {len(result)} records\")\n",
    "                return result\n",
    "            else:\n",
    "                print(\"⚠️ Empty result, trying next strategy...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Attempt {attempt} failed: {e}\")\n",
    "            if attempt < len(strategies):\n",
    "                print(\"🔄 Trying next strategy...\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Generate the dataset\n",
    "print(\"🚀 Starting data generation...\")\n",
    "synthetic_df = generate_qa_dataset_robust(navigator, PROMPT, source_text)\n",
    "\n",
    "if synthetic_df is not None:\n",
    "    print(f\"\\n📊 Dataset generated successfully!\")\n",
    "    print(f\"   Shape: {synthetic_df.shape}\")\n",
    "    print(f\"   Columns: {list(synthetic_df.columns)}\")\n",
    "    \n",
    "    # Display sample data\n",
    "    pd.set_option('display.max_colwidth', 100)\n",
    "    print(f\"\\n📋 Sample generated data:\")\n",
    "    print(synthetic_df.head(3))\n",
    "else:\n",
    "    print(\"❌ Failed to generate any data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60bf351",
   "metadata": {},
   "source": [
    "## 🔄 Convert Gretel Format to Opik Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0e5a423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Starting format conversion...\n",
      "🔄 Converting 10 rows to Opik format...\n",
      "📝 Available columns: ['question', 'context', 'truth']\n",
      "✅ Question column: question\n",
      "✅ Answer column: truth\n",
      "✅ Context column: context\n",
      "✅ Successfully converted 10/10 rows\n",
      "\n",
      "📋 Conversion completed successfully!\n",
      "📊 Converted 10 items\n",
      "\n",
      "📋 Sample converted item:\n",
      "{\n",
      "  \"input\": {\n",
      "    \"question\": \"What is the primary function of machine learning in AI?\",\n",
      "    \"context\": \"Machine learning, a subset of AI, enables systems to learn from data without explicit programming.\"\n",
      "  },\n",
      "  \"expected_output\": \"Machine learning allows systems to learn from data without explicit programming, enabling them to improve their performance on tasks over time.\",\n",
      "  \"metadata\": {\n",
      "    \"source\": \"gretel_navigator\",\n",
      "    \"generated\": true,\n",
      "    \"row_index\": 0,\n",
      "    \"model\": \"gretelai/auto\",\n",
      "    \"question_length\": 55,\n",
      "    \"answer_length\": 143,\n",
      "    \"has_context\": true\n",
      "  }\n",
      "}\n",
      "\n",
      "✅ Converted data is now available globally\n",
      "\n",
      "🎯 Conversion step completed!\n",
      "Next: Run Cell 7 to push data to Opik\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 - Convert Gretel Format to Opik Format\n",
    "\n",
    "def convert_gretel_to_opik_format(df, model_name=\"unknown\"):\n",
    "    \"\"\"Enhanced conversion with better validation and error handling\"\"\"\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        raise ValueError(\"Input DataFrame is None or empty\")\n",
    "    \n",
    "    print(f\"🔄 Converting {len(df)} rows to Opik format...\")\n",
    "    print(f\"📝 Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Enhanced column detection\n",
    "    column_mapping = {\n",
    "        'question': ['question', 'query', 'q', 'prompt'],\n",
    "        'answer': ['truth', 'answer', 'response', 'reply', 'a'],\n",
    "        'context': ['context', 'background', 'source', 'passage']\n",
    "    }\n",
    "    \n",
    "    detected_columns = {}\n",
    "    for opik_col, possible_names in column_mapping.items():\n",
    "        for col in df.columns:\n",
    "            if any(name in col.lower().strip() for name in possible_names):\n",
    "                detected_columns[opik_col] = col\n",
    "                print(f\"✅ {opik_col.title()} column: {col}\")\n",
    "                break\n",
    "    \n",
    "    # Validate required columns\n",
    "    if 'question' not in detected_columns:\n",
    "        raise ValueError(\"No question column found\")\n",
    "    if 'answer' not in detected_columns:\n",
    "        raise ValueError(\"No answer column found\")\n",
    "    \n",
    "    opik_items = []\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            # Build input with validation\n",
    "            input_data = {}\n",
    "            \n",
    "            question = str(row[detected_columns['question']]).strip() if pd.notna(row.get(detected_columns['question'])) else \"\"\n",
    "            if not question or len(question) < 5:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            input_data[\"question\"] = question\n",
    "            \n",
    "            if 'context' in detected_columns and pd.notna(row.get(detected_columns['context'])):\n",
    "                context = str(row[detected_columns['context']]).strip()\n",
    "                if context:\n",
    "                    input_data[\"context\"] = context\n",
    "            \n",
    "            # Get expected output with validation\n",
    "            answer = str(row[detected_columns['answer']]).strip() if pd.notna(row.get(detected_columns['answer'])) else \"\"\n",
    "            if not answer or len(answer) < 10:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Create Opik item\n",
    "            item = {\n",
    "                \"input\": input_data,\n",
    "                \"expected_output\": answer,\n",
    "                \"metadata\": {\n",
    "                    \"source\": \"gretel_navigator\",\n",
    "                    \"generated\": True,\n",
    "                    \"row_index\": idx,\n",
    "                    \"model\": model_name,\n",
    "                    \"question_length\": len(question),\n",
    "                    \"answer_length\": len(answer),\n",
    "                    \"has_context\": \"context\" in input_data\n",
    "                }\n",
    "            }\n",
    "            opik_items.append(item)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error converting row {idx}: {e}\")\n",
    "            skipped_count += 1\n",
    "    \n",
    "    print(f\"✅ Successfully converted {len(opik_items)}/{len(df)} rows\")\n",
    "    if skipped_count > 0:\n",
    "        print(f\"⚠️ Skipped {skipped_count} rows due to validation issues\")\n",
    "    \n",
    "    return opik_items\n",
    "\n",
    "# Execute the conversion\n",
    "print(\"🔄 Starting format conversion...\")\n",
    "\n",
    "# Check if synthetic_df exists and has data\n",
    "if 'synthetic_df' in locals() and synthetic_df is not None and len(synthetic_df) > 0:\n",
    "    try:\n",
    "        # Convert the dataset\n",
    "        opik_formatted_data = convert_gretel_to_opik_format(synthetic_df, working_model)\n",
    "        \n",
    "        if opik_formatted_data:\n",
    "            print(f\"\\n📋 Conversion completed successfully!\")\n",
    "            print(f\"📊 Converted {len(opik_formatted_data)} items\")\n",
    "            print(f\"\\n📋 Sample converted item:\")\n",
    "            print(json.dumps(opik_formatted_data[0], indent=2))\n",
    "            \n",
    "            # Store in global scope for next cells\n",
    "            globals()['opik_formatted_data'] = opik_formatted_data\n",
    "            print(f\"\\n✅ Converted data is now available globally\")\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ No items were successfully converted\")\n",
    "            globals()['opik_formatted_data'] = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Conversion failed: {e}\")\n",
    "        print(\"\\n🔧 Troubleshooting:\")\n",
    "        print(\"   1. Check if synthetic_df has the expected columns\")\n",
    "        print(\"   2. Verify data quality (non-empty questions/answers)\")\n",
    "        print(\"   3. Re-run data generation if needed\")\n",
    "        globals()['opik_formatted_data'] = None\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No synthetic data available to convert\")\n",
    "    print(\"🔄 Available variables:\")\n",
    "    \n",
    "    # Debug information\n",
    "    debug_vars = ['synthetic_df', 'navigator', 'working_model']\n",
    "    for var in debug_vars:\n",
    "        if var in locals():\n",
    "            val = locals()[var]\n",
    "            print(f\"   ✅ {var}: {type(val).__name__} ({len(val) if hasattr(val, '__len__') else 'N/A'})\")\n",
    "        elif var in globals():\n",
    "            val = globals()[var]\n",
    "            print(f\"   ✅ {var}: {type(val).__name__} (global)\")\n",
    "        else:\n",
    "            print(f\"   ❌ {var}: Not found\")\n",
    "    \n",
    "    print(\"\\n📋 Please ensure:\")\n",
    "    print(\"   1. Cell 3 (find working model) completed successfully\")\n",
    "    print(\"   2. Cell 5 (data generation) completed successfully\")\n",
    "    print(\"   3. synthetic_df variable contains valid data\")\n",
    "    \n",
    "    globals()['opik_formatted_data'] = None\n",
    "\n",
    "print(f\"\\n🎯 Conversion step completed!\")\n",
    "print(f\"Next: Run Cell 7 to push data to Opik\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dbef7e",
   "metadata": {},
   "source": [
    "## 📤 Push Dataset to Opik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf008047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📤 Starting push to Opik...\n",
      "📊 Data to push: 10 items\n",
      "🤖 Model used: gretelai/auto\n",
      "📤 Pushing 10 items to Opik...\n",
      "📊 Creating/finding dataset: gretel-ai-qa-cookbook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Created a \"gretel-ai-qa-cookbook\" dataset at https://www.comet.com/opik/api/v1/session/redirect/datasets/?dataset_id=0197b141-e92a-7bbe-9a1c-6e2b909a3536&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset ready: gretel-ai-qa-cookbook\n",
      "🆔 Dataset ID: 0197b141-e92a-7bbe-9a1c-6e2b909a3536\n",
      "📥 Inserting data into Opik...\n",
      "⏳ Waiting for data propagation...\n",
      "✅ Successfully pushed 10 items!\n",
      "📊 Dataset name: gretel-ai-qa-cookbook\n",
      "🆔 Dataset ID: 0197b141-e92a-7bbe-9a1c-6e2b909a3536\n",
      "\n",
      "📋 Sample item pushed:\n",
      "   📝 Question: What is the primary function of machine learning in AI?...\n",
      "   💡 Answer: Machine learning allows systems to learn from data without explicit programming,...\n",
      "   📊 Metadata keys: ['source', 'generated', 'row_index', 'model', 'question_length', 'answer_length', 'has_context']\n",
      "\n",
      "📈 Dataset statistics:\n",
      "   📝 Total questions: 10\n",
      "   📄 Items with context: 10/10 (100.0%)\n",
      "   📏 Average question length: 64.9 characters\n",
      "   📏 Average answer length: 140.1 characters\n",
      "\n",
      "🎉 Integration completed successfully!\n",
      "📊 Dataset 'gretel-ai-qa-cookbook' is now available in Opik\n",
      "\n",
      "🔗 Next steps:\n",
      "   1. Go to your Comet workspace\n",
      "   2. Navigate to Opik → Datasets\n",
      "   3. Find your dataset: gretel-ai-qa-cookbook\n",
      "   4. Use it in model evaluations!\n",
      "   5. Run Cell 8 to verify the dataset\n",
      "\n",
      "🎯 Push operation completed!\n",
      "   Success: True\n",
      "   Result: gretel-ai-qa-cookbook\n",
      "   Next: Run Cell 8 to verify the dataset\n",
      "\n",
      "🔍 Variables available for verification:\n",
      "   ✅ success: True\n",
      "   ✅ result: gretel-ai-qa-cookbook\n",
      "   ✅ opik_formatted_data: 10 items\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 - Push Dataset to Opik\n",
    "\n",
    "import time  # Required for retry delays\n",
    "\n",
    "def push_to_opik(opik_data, dataset_name=\"gretel-qa-dataset\", max_retries=3):\n",
    "    \"\"\"Enhanced push with retry logic and better error handling\"\"\"\n",
    "    \n",
    "    if not opik_data:\n",
    "        return False, \"No data to push\"\n",
    "    \n",
    "    print(f\"📤 Pushing {len(opik_data)} items to Opik...\")\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Import and initialize Opik client\n",
    "            import opik\n",
    "            opik_client = opik.Opik()\n",
    "            \n",
    "            # Create or get dataset with enhanced description\n",
    "            description = (\n",
    "                f\"Synthetic Q&A dataset generated using Gretel Navigator ({working_model}). \"\n",
    "                f\"Contains {len(opik_data)} items with questions, contexts, and expected answers. \"\n",
    "                f\"Generated on {time.strftime('%Y-%m-%d %H:%M:%S')}.\"\n",
    "            )\n",
    "            \n",
    "            print(f\"📊 Creating/finding dataset: {dataset_name}\")\n",
    "            opik_dataset = opik_client.get_or_create_dataset(\n",
    "                name=dataset_name,\n",
    "                description=description\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ Dataset ready: {opik_dataset.name}\")\n",
    "            print(f\"🆔 Dataset ID: {opik_dataset.id}\")\n",
    "            \n",
    "            # Insert data with progress indication\n",
    "            print(\"📥 Inserting data into Opik...\")\n",
    "            opik_dataset.insert(opik_data)\n",
    "            \n",
    "            # Brief wait for data propagation\n",
    "            print(\"⏳ Waiting for data propagation...\")\n",
    "            time.sleep(3)\n",
    "            \n",
    "            print(f\"✅ Successfully pushed {len(opik_data)} items!\")\n",
    "            print(f\"📊 Dataset name: {opik_dataset.name}\")\n",
    "            print(f\"🆔 Dataset ID: {opik_dataset.id}\")\n",
    "            \n",
    "            # Show detailed sample of what was pushed\n",
    "            if opik_data:\n",
    "                sample = opik_data[0]\n",
    "                print(f\"\\n📋 Sample item pushed:\")\n",
    "                print(f\"   📝 Question: {sample['input'].get('question', 'N/A')[:80]}...\")\n",
    "                print(f\"   💡 Answer: {sample['expected_output'][:80]}...\")\n",
    "                print(f\"   📊 Metadata keys: {list(sample['metadata'].keys())}\")\n",
    "                \n",
    "                # Show statistics\n",
    "                total_questions = len(opik_data)\n",
    "                has_context = sum(1 for item in opik_data if 'context' in item['input'])\n",
    "                avg_q_len = sum(len(item['input'].get('question', '')) for item in opik_data) / total_questions\n",
    "                avg_a_len = sum(len(item['expected_output']) for item in opik_data) / total_questions\n",
    "                \n",
    "                print(f\"\\n📈 Dataset statistics:\")\n",
    "                print(f\"   📝 Total questions: {total_questions}\")\n",
    "                print(f\"   📄 Items with context: {has_context}/{total_questions} ({has_context/total_questions*100:.1f}%)\")\n",
    "                print(f\"   📏 Average question length: {avg_q_len:.1f} characters\")\n",
    "                print(f\"   📏 Average answer length: {avg_a_len:.1f} characters\")\n",
    "            \n",
    "            return True, opik_dataset.name\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"❌ Attempt {attempt + 1}/{max_retries} failed: {error_msg[:150]}...\")\n",
    "            \n",
    "            # Provide specific error guidance\n",
    "            if \"authentication\" in error_msg.lower():\n",
    "                print(\"🔐 Authentication issue - try running: opik.configure()\")\n",
    "            elif \"network\" in error_msg.lower() or \"connection\" in error_msg.lower():\n",
    "                print(\"🌐 Network issue - check your internet connection\")\n",
    "            elif \"quota\" in error_msg.lower() or \"limit\" in error_msg.lower():\n",
    "                print(\"📊 Rate limit or quota issue - wait before retrying\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = (attempt + 1) * 3  # Progressive backoff\n",
    "                print(f\"🔄 Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"❌ All {max_retries} attempts failed\")\n",
    "                print(\"\\n🔧 Troubleshooting steps:\")\n",
    "                print(\"   1. Check Opik authentication: opik.configure()\")\n",
    "                print(\"   2. Verify internet connection\")\n",
    "                print(\"   3. Check Comet workspace access\")\n",
    "                print(\"   4. Try with a smaller dataset first\")\n",
    "                return False, error_msg\n",
    "\n",
    "    return False, \"Max retries exceeded\"\n",
    "\n",
    "# Execute the push operation\n",
    "print(\"📤 Starting push to Opik...\")\n",
    "\n",
    "# Validate prerequisites\n",
    "prerequisites_ok = True\n",
    "missing_items = []\n",
    "\n",
    "# Check if opik_formatted_data exists\n",
    "if 'opik_formatted_data' not in locals() and 'opik_formatted_data' not in globals():\n",
    "    missing_items.append(\"opik_formatted_data\")\n",
    "    prerequisites_ok = False\n",
    "else:\n",
    "    # Get from appropriate scope\n",
    "    if 'opik_formatted_data' in locals():\n",
    "        data_to_push = opik_formatted_data\n",
    "    else:\n",
    "        data_to_push = globals()['opik_formatted_data']\n",
    "    \n",
    "    if not data_to_push:\n",
    "        missing_items.append(\"valid opik_formatted_data\")\n",
    "        prerequisites_ok = False\n",
    "\n",
    "# Check if working_model exists\n",
    "if 'working_model' not in locals() and 'working_model' not in globals():\n",
    "    missing_items.append(\"working_model\")\n",
    "    prerequisites_ok = False\n",
    "\n",
    "if not prerequisites_ok:\n",
    "    print(f\"❌ Missing prerequisites: {missing_items}\")\n",
    "    print(\"📋 Please ensure these previous steps completed successfully:\")\n",
    "    print(\"   1. Cell 3: Find working model\")\n",
    "    print(\"   2. Cell 5: Data generation\") \n",
    "    print(\"   3. Cell 6: Format conversion\")\n",
    "    \n",
    "    # Set failure states\n",
    "    success = False\n",
    "    result = None\n",
    "    \n",
    "else:\n",
    "    # Execute the push\n",
    "    try:\n",
    "        # Get the data from the appropriate scope\n",
    "        if 'opik_formatted_data' in locals():\n",
    "            data_to_push = opik_formatted_data\n",
    "        else:\n",
    "            data_to_push = globals()['opik_formatted_data']\n",
    "        \n",
    "        # Get working model\n",
    "        if 'working_model' in locals():\n",
    "            model_name = working_model\n",
    "        else:\n",
    "            model_name = globals()['working_model']\n",
    "        \n",
    "        print(f\"📊 Data to push: {len(data_to_push)} items\")\n",
    "        print(f\"🤖 Model used: {model_name}\")\n",
    "        \n",
    "        # Push to Opik\n",
    "        success, result = push_to_opik(data_to_push, \"gretel-ai-qa-cookbook\")\n",
    "        \n",
    "        if success:\n",
    "            print(f\"\\n🎉 Integration completed successfully!\")\n",
    "            print(f\"📊 Dataset '{result}' is now available in Opik\")\n",
    "            print(f\"\\n🔗 Next steps:\")\n",
    "            print(f\"   1. Go to your Comet workspace\")\n",
    "            print(f\"   2. Navigate to Opik → Datasets\")\n",
    "            print(f\"   3. Find your dataset: {result}\")\n",
    "            print(f\"   4. Use it in model evaluations!\")\n",
    "            print(f\"   5. Run Cell 8 to verify the dataset\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\n❌ Failed to complete integration\")\n",
    "            print(f\"💡 Error details: {result}\")\n",
    "            print(f\"\\n🔧 Try these solutions:\")\n",
    "            print(f\"   1. Check your Opik authentication\")\n",
    "            print(f\"   2. Verify network connectivity\")\n",
    "            print(f\"   3. Try with a smaller dataset\")\n",
    "            print(f\"   4. Contact support if issue persists\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Unexpected error during push: {e}\")\n",
    "        success = False\n",
    "        result = str(e)\n",
    "\n",
    "# Store results in global scope for verification step\n",
    "globals()['success'] = success\n",
    "globals()['result'] = result\n",
    "if 'data_to_push' in locals():\n",
    "    globals()['opik_formatted_data'] = data_to_push\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n🎯 Push operation completed!\")\n",
    "print(f\"   Success: {success}\")\n",
    "print(f\"   Result: {result}\")\n",
    "print(f\"   Next: Run Cell 8 to verify the dataset\")\n",
    "\n",
    "# Debug information\n",
    "print(f\"\\n🔍 Variables available for verification:\")\n",
    "for var_name in ['success', 'result', 'opik_formatted_data']:\n",
    "    if var_name in globals():\n",
    "        val = globals()[var_name]\n",
    "        if var_name == 'opik_formatted_data':\n",
    "            print(f\"   ✅ {var_name}: {len(val) if val else 0} items\")\n",
    "        else:\n",
    "            print(f\"   ✅ {var_name}: {val}\")\n",
    "    else:\n",
    "        print(f\"   ❌ {var_name}: Not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4848097",
   "metadata": {},
   "source": [
    "The gretel-qa-dataset dataset can now be viewed in the UI:\n",
    "\n",
    "![gretel-qa-dataset](https://raw.githubusercontent.com/comet-ml/opik/main/apps/opik-documentation/documentation/fern/img/cookbook/gretel_opik_integration_cookbook.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f00fd",
   "metadata": {},
   "source": [
    "## ✅ Verify Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "812e4a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Starting dataset verification...\n",
      "🎯 Verifying dataset: gretel-ai-qa-cookbook\n",
      "🔍 Verifying dataset: gretel-ai-qa-cookbook\n",
      "✅ Dataset verified: gretel-ai-qa-cookbook\n",
      "🆔 Dataset ID: 0197b141-e92a-7bbe-9a1c-6e2b909a3536\n",
      "📊 Fetching dataset details...\n",
      "📋 Dataset successfully created and accessible\n",
      "\n",
      "📋 How to view your dataset:\n",
      "   1. Go to https://www.comet.com\n",
      "   2. Navigate to your workspace\n",
      "   3. Click on 'Opik' in the left sidebar\n",
      "   4. Go to 'Datasets' tab\n",
      "   5. Look for dataset: gretel-ai-qa-cookbook\n",
      "\n",
      "🧪 How to use in evaluations:\n",
      "📝 Basic Usage:\n",
      "\n",
      "# Basic Usage Example:\n",
      "import opik\n",
      "\n",
      "# Initialize client\n",
      "opik_client = opik.Opik()\n",
      "dataset = opik_client.get_dataset('gretel-ai-qa-cookbook')\n",
      "\n",
      "@opik.track\n",
      "def my_qa_model(input_data):\n",
      "    question = input_data.get('question', '')\n",
      "    context = input_data.get('context', '')\n",
      "    \n",
      "    # Your model logic here (example with simple response)\n",
      "    if context:\n",
      "        response = f\"Based on the context: {context[:100]}..., the answer to '{question}' is...\"\n",
      "    else:\n",
      "        response = f\"The answer to '{question}' is...\"\n",
      "    \n",
      "    return response\n",
      "\n",
      "# Run evaluation\n",
      "evaluation = opik.evaluate(\n",
      "    dataset=dataset,\n",
      "    task=my_qa_model,\n",
      "    experiment_name=\"gretel-synthetic-eval-{int(time.time())}\"\n",
      ")\n",
      "\n",
      "print(f\"Evaluation completed: {evaluation}\")\n",
      "\n",
      "\n",
      "📊 Advanced Usage with Metrics:\n",
      "\n",
      "# Advanced Usage with Custom Metrics:\n",
      "import opik\n",
      "from opik.evaluation.metrics import Equals, Contains\n",
      "\n",
      "opik_client = opik.Opik()\n",
      "dataset = opik_client.get_dataset('gretel-ai-qa-cookbook')\n",
      "\n",
      "@opik.track\n",
      "def advanced_qa_model(input_data):\n",
      "    question = input_data.get('question', '')\n",
      "    context = input_data.get('context', '')\n",
      "    \n",
      "    # Your advanced model logic here\n",
      "    # Could integrate with OpenAI, Anthropic, or local models\n",
      "    response = \"Your model's detailed response here\"\n",
      "    \n",
      "    return response\n",
      "\n",
      "# Define custom metrics\n",
      "def semantic_similarity_metric(output, expected_output):\n",
      "    # Implement semantic similarity check\n",
      "    # Could use sentence transformers, embeddings, etc.\n",
      "    return {\"score\": 0.85, \"reason\": \"High semantic similarity\"}\n",
      "\n",
      "# Run evaluation with metrics\n",
      "evaluation = opik.evaluate(\n",
      "    dataset=dataset,\n",
      "    task=advanced_qa_model,\n",
      "    scoring_metrics=[\n",
      "        Equals(),  # Exact match\n",
      "        Contains(),  # Substring match\n",
      "        semantic_similarity_metric  # Custom metric\n",
      "    ],\n",
      "    experiment_name=\"gretel-advanced-eval-{int(time.time())}\"\n",
      ")\n",
      "\n",
      "\n",
      "🔗 Additional Resources:\n",
      "   📖 Opik Documentation: https://opik.comet.com/\n",
      "   🧪 Evaluation Examples: https://opik.comet.com/evaluation/\n",
      "   🤖 Model Integration: https://opik.comet.com/integrations/\n",
      "\n",
      "🎉 Verification completed successfully!\n",
      "📊 Dataset 'gretel-ai-qa-cookbook' is ready for use!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 - Enhanced Dataset Verification\n",
    "\n",
    "def verify_opik_dataset(dataset_name):\n",
    "    \"\"\"Enhanced dataset verification with detailed information and error handling\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"🔍 Verifying dataset: {dataset_name}\")\n",
    "        \n",
    "        # Import opik with error handling\n",
    "        try:\n",
    "            import opik\n",
    "            opik_client = opik.Opik()\n",
    "        except ImportError:\n",
    "            print(\"❌ Opik not installed. Run: pip install opik\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to initialize Opik client: {e}\")\n",
    "            print(\"💡 Try running: opik.configure() first\")\n",
    "            return False\n",
    "        \n",
    "        # Get the dataset with retry logic\n",
    "        max_retries = 3\n",
    "        dataset = None\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                dataset = opik_client.get_dataset(dataset_name)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"⚠️ Attempt {attempt + 1} failed, retrying...\")\n",
    "                    time.sleep(2)\n",
    "                else:\n",
    "                    raise e\n",
    "        \n",
    "        if dataset is None:\n",
    "            print(f\"❌ Dataset '{dataset_name}' not found\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"✅ Dataset verified: {dataset.name}\")\n",
    "        print(f\"🆔 Dataset ID: {dataset.id}\")\n",
    "        \n",
    "        # Try to get additional dataset information\n",
    "        try:\n",
    "            # Get dataset items to show count and sample\n",
    "            print(f\"📊 Fetching dataset details...\")\n",
    "            \n",
    "            # Note: Actual item counting depends on Opik API capabilities\n",
    "            print(f\"📋 Dataset successfully created and accessible\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not fetch dataset details: {e}\")\n",
    "        \n",
    "        # Enhanced usage instructions\n",
    "        print(f\"\\n📋 How to view your dataset:\")\n",
    "        print(f\"   1. Go to https://www.comet.com\")\n",
    "        print(f\"   2. Navigate to your workspace\")\n",
    "        print(f\"   3. Click on 'Opik' in the left sidebar\")\n",
    "        print(f\"   4. Go to 'Datasets' tab\")\n",
    "        print(f\"   5. Look for dataset: {dataset_name}\")\n",
    "        \n",
    "        print(f\"\\n🧪 How to use in evaluations:\")\n",
    "        \n",
    "        # Basic usage example\n",
    "        basic_example = f'''\n",
    "# Basic Usage Example:\n",
    "import opik\n",
    "\n",
    "# Initialize client\n",
    "opik_client = opik.Opik()\n",
    "dataset = opik_client.get_dataset('{dataset_name}')\n",
    "\n",
    "@opik.track\n",
    "def my_qa_model(input_data):\n",
    "    question = input_data.get('question', '')\n",
    "    context = input_data.get('context', '')\n",
    "    \n",
    "    # Your model logic here (example with simple response)\n",
    "    if context:\n",
    "        response = f\"Based on the context: {{context[:100]}}..., the answer to '{{question}}' is...\"\n",
    "    else:\n",
    "        response = f\"The answer to '{{question}}' is...\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Run evaluation\n",
    "evaluation = opik.evaluate(\n",
    "    dataset=dataset,\n",
    "    task=my_qa_model,\n",
    "    experiment_name=\"gretel-synthetic-eval-{{int(time.time())}}\"\n",
    ")\n",
    "\n",
    "print(f\"Evaluation completed: {{evaluation}}\")\n",
    "'''\n",
    "        \n",
    "        # Advanced usage with metrics\n",
    "        advanced_example = f'''\n",
    "# Advanced Usage with Custom Metrics:\n",
    "import opik\n",
    "from opik.evaluation.metrics import Equals, Contains\n",
    "\n",
    "opik_client = opik.Opik()\n",
    "dataset = opik_client.get_dataset('{dataset_name}')\n",
    "\n",
    "@opik.track\n",
    "def advanced_qa_model(input_data):\n",
    "    question = input_data.get('question', '')\n",
    "    context = input_data.get('context', '')\n",
    "    \n",
    "    # Your advanced model logic here\n",
    "    # Could integrate with OpenAI, Anthropic, or local models\n",
    "    response = \"Your model's detailed response here\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Define custom metrics\n",
    "def semantic_similarity_metric(output, expected_output):\n",
    "    # Implement semantic similarity check\n",
    "    # Could use sentence transformers, embeddings, etc.\n",
    "    return {{\"score\": 0.85, \"reason\": \"High semantic similarity\"}}\n",
    "\n",
    "# Run evaluation with metrics\n",
    "evaluation = opik.evaluate(\n",
    "    dataset=dataset,\n",
    "    task=advanced_qa_model,\n",
    "    scoring_metrics=[\n",
    "        Equals(),  # Exact match\n",
    "        Contains(),  # Substring match\n",
    "        semantic_similarity_metric  # Custom metric\n",
    "    ],\n",
    "    experiment_name=\"gretel-advanced-eval-{{int(time.time())}}\"\n",
    ")\n",
    "'''\n",
    "        \n",
    "        print(\"📝 Basic Usage:\")\n",
    "        print(basic_example)\n",
    "        \n",
    "        print(\"\\n📊 Advanced Usage with Metrics:\")\n",
    "        print(advanced_example)\n",
    "        \n",
    "        print(f\"\\n🔗 Additional Resources:\")\n",
    "        print(f\"   📖 Opik Documentation: https://opik.comet.com/\")\n",
    "        print(f\"   🧪 Evaluation Examples: https://opik.comet.com/evaluation/\")\n",
    "        print(f\"   🤖 Model Integration: https://opik.comet.com/integrations/\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Could not verify dataset: {e}\")\n",
    "        \n",
    "        # Provide troubleshooting guidance\n",
    "        print(f\"\\n🔧 Troubleshooting:\")\n",
    "        print(f\"   1. Check if dataset name is correct: '{dataset_name}'\")\n",
    "        print(f\"   2. Verify Opik authentication: opik.configure()\")\n",
    "        print(f\"   3. Check if previous push step completed successfully\")\n",
    "        print(f\"   4. Try refreshing your Comet workspace\")\n",
    "        \n",
    "        # Check if variables exist for debugging\n",
    "        debug_info = {}\n",
    "        for var_name in ['opik_formatted_data', 'success', 'result']:\n",
    "            if var_name in globals():\n",
    "                debug_info[var_name] = f\"Available ({type(globals()[var_name]).__name__})\"\n",
    "            else:\n",
    "                debug_info[var_name] = \"Not available\"\n",
    "        \n",
    "        print(f\"\\n🐛 Debug information:\")\n",
    "        for var, status in debug_info.items():\n",
    "            print(f\"   {var}: {status}\")\n",
    "        \n",
    "        return False\n",
    "\n",
    "def check_verification_prerequisites():\n",
    "    \"\"\"Check if all required variables exist for verification\"\"\"\n",
    "    required_vars = {\n",
    "        'success': 'success' in locals() or 'success' in globals(),\n",
    "        'result': 'result' in locals() or 'result' in globals(),\n",
    "        'opik_formatted_data': 'opik_formatted_data' in locals() or 'opik_formatted_data' in globals()\n",
    "    }\n",
    "    \n",
    "    missing_vars = [var for var, exists in required_vars.items() if not exists]\n",
    "    \n",
    "    if missing_vars:\n",
    "        print(f\"⚠️ Missing variables for verification: {missing_vars}\")\n",
    "        print(\"🔄 This usually means previous steps didn't complete successfully\")\n",
    "        return False, missing_vars\n",
    "    \n",
    "    return True, []\n",
    "\n",
    "# Main verification execution\n",
    "print(\"🔍 Starting dataset verification...\")\n",
    "\n",
    "# Check prerequisites\n",
    "prereq_check, missing = check_verification_prerequisites()\n",
    "\n",
    "if not prereq_check:\n",
    "    print(f\"❌ Cannot verify dataset - missing prerequisites: {missing}\")\n",
    "    print(\"📋 Please ensure these previous steps completed successfully:\")\n",
    "    print(\"   1. Cell 5: Data generation\")\n",
    "    print(\"   2. Cell 6: Format conversion\") \n",
    "    print(\"   3. Cell 7: Push to Opik\")\n",
    "else:\n",
    "    # Try to get the dataset name from previous results\n",
    "    dataset_name_to_verify = None\n",
    "    \n",
    "    # Check for result from push operation\n",
    "    if 'result' in locals() and 'success' in locals() and success:\n",
    "        dataset_name_to_verify = result\n",
    "    elif 'result' in globals() and 'success' in globals() and globals()['success']:\n",
    "        dataset_name_to_verify = globals()['result']\n",
    "    else:\n",
    "        # Fallback to default name\n",
    "        dataset_name_to_verify = \"gretel-ai-qa-cookbook\"\n",
    "        print(f\"⚠️ Using default dataset name: {dataset_name_to_verify}\")\n",
    "    \n",
    "    if dataset_name_to_verify:\n",
    "        print(f\"🎯 Verifying dataset: {dataset_name_to_verify}\")\n",
    "        verification_success = verify_opik_dataset(dataset_name_to_verify)\n",
    "        \n",
    "        if verification_success:\n",
    "            print(f\"\\n🎉 Verification completed successfully!\")\n",
    "            print(f\"📊 Dataset '{dataset_name_to_verify}' is ready for use!\")\n",
    "        else:\n",
    "            print(f\"\\n❌ Verification failed for dataset '{dataset_name_to_verify}'\")\n",
    "    else:\n",
    "        print(\"❌ No dataset name available for verification\")\n",
    "        print(\"🔄 Please run the previous cells to generate and push a dataset first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d592e5",
   "metadata": {},
   "source": [
    "## 🔄 Alternative: Load from Gretel Export Files\n",
    "\n",
    "If you have pre-existing Gretel datasets exported as files, you can also import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e57d9ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gretel_export(file_path, format_type=\"csv\"):\n",
    "    \"\"\"\n",
    "    Load a Gretel dataset export from local file.\n",
    "    Supports CSV, JSON, and JSONL formats.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if format_type.lower() == \"csv\":\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif format_type.lower() == \"json\":\n",
    "            df = pd.read_json(file_path)\n",
    "        elif format_type.lower() == \"jsonl\":\n",
    "            df = pd.read_json(file_path, lines=True)\n",
    "        else:\n",
    "            raise ValueError(\"Supported formats: csv, json, jsonl\")\n",
    "        \n",
    "        print(f\"✅ Loaded {len(df)} records from {file_path}\")\n",
    "        print(f\"📊 Dataset shape: {df.shape}\")\n",
    "        print(f\"📋 Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Display sample data from Gretel\n",
    "        print(\"\\n📄 Sample data from Gretel:\")\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.max_colwidth', 100)\n",
    "        print(df.head(3))\n",
    "        \n",
    "        # Show data types\n",
    "        print(f\"\\n📈 Data types:\")\n",
    "        print(df.dtypes)\n",
    "        \n",
    "        # Basic statistics\n",
    "        print(f\"\\n📊 Basic statistics:\")\n",
    "        if 'question' in df.columns:\n",
    "            print(f\"  - Average question length: {df['question'].str.len().mean():.1f} characters\")\n",
    "        if 'answer' in df.columns or 'truth' in df.columns:\n",
    "            answer_col = 'answer' if 'answer' in df.columns else 'truth'\n",
    "            print(f\"  - Average answer length: {df[answer_col].str.len().mean():.1f} characters\")\n",
    "        if 'topic' in df.columns:\n",
    "            print(f\"  - Unique topics: {df['topic'].nunique()}\")\n",
    "        if 'difficulty' in df.columns or 'user_profile' in df.columns:\n",
    "            diff_col = 'difficulty' if 'difficulty' in df.columns else 'user_profile'\n",
    "            print(f\"  - Difficulty distribution: {dict(df[diff_col].value_counts())}\")\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "# df_gretel = load_gretel_export(\"your_gretel_export.csv\", \"csv\")\n",
    "# df_gretel = load_gretel_export(\"your_gretel_export.jsonl\", \"jsonl\")\n",
    "\n",
    "# Then convert and push to Opik:\n",
    "# opik_data = convert_gretel_to_opik_format(df_gretel, \"gretel-export\")\n",
    "# success, result = push_to_opik(opik_data, \"gretel-imported-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231cec81",
   "metadata": {},
   "source": [
    "## 🎯 Complete Integration Summary\n",
    "\n",
    "This cookbook provides a complete workflow for integrating Gretel AI datasets with Opik:\n",
    "\n",
    "### ✅ **What We Accomplished:**\n",
    "1. **Authentication Setup** - Both Gretel and Opik API configurations\n",
    "2. **Model Discovery** - Automatic detection of working Gretel models\n",
    "3. **Synthetic Data Generation** - Using Gretel Navigator for Q&A creation\n",
    "4. **Format Conversion** - Transform Gretel output to Opik-compatible format\n",
    "5. **Dataset Import** - Push datasets to Opik for evaluation use\n",
    "6. **Verification** - Confirm successful import and provide usage guidance\n",
    "\n",
    "### 🔧 **Key Features:**\n",
    "- **Robust Error Handling**: Multiple fallback strategies\n",
    "- **Automatic Column Detection**: Smart mapping of data fields\n",
    "- **Flexible Input**: Supports both live generation and file imports\n",
    "- **Production Ready**: Comprehensive validation and user guidance\n",
    "\n",
    "### 📊 **Use Cases:**\n",
    "- **Model Testing**: Create evaluation datasets for Q&A models\n",
    "- **Benchmarking**: Generate consistent test sets across experiments\n",
    "- **Agent Optimization**: Provide training data for Opik's Agent Optimizer\n",
    "- **Continuous Evaluation**: Regular model performance monitoring\n",
    "\n",
    "### 🚀 **Next Steps:**\n",
    "1. Customize the `source_text` with your domain-specific content\n",
    "2. Adjust generation parameters based on your needs\n",
    "3. Use the imported dataset in Opik evaluations\n",
    "4. Scale up for larger dataset generation\n",
    "\n",
    "This integration enables seamless data flow from Gretel's synthetic data generation capabilities into Opik's model evaluation and optimization ecosystem! 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
