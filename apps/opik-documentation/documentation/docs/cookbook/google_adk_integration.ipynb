{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4051c6a8",
   "metadata": {},
   "source": [
    "# Opik + Google ADK Integration Cookbook\n",
    "\n",
    "This cookbook demonstrates how to integrate Google's Agent Development Kit (ADK) with Opik for comprehensive observability and tracing of your AI agents.\n",
    "\n",
    "## 🎯 What You'll Learn\n",
    "\n",
    "- How to set up Opik with Google ADK\n",
    "- Building traced agents with function calling capabilities\n",
    "- Creating multi-agent systems with observability\n",
    "- Best practices for debugging and monitoring ADK agents\n",
    "\n",
    "## 📋 Prerequisites\n",
    "\n",
    "- Python 3.10+\n",
    "- Google Cloud Project with Vertex AI API enabled\n",
    "- Opik account (free at [comet.com](https://comet.com))\n",
    "\n",
    "## 📚 Table of Contents\n",
    "\n",
    "1. [Environment Setup](#environment-setup)\n",
    "2. [Basic ADK Agent with Opik](#basic-adk-agent-with-opik)\n",
    "3. [Function Calling with Tracing](#function-calling-with-tracing)\n",
    "4. [Multi-Agent Systems](#multi-agent-systems)\n",
    "5. [Advanced Debugging](#advanced-debugging)\n",
    "6. [Best Practices](#best-practices)\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "### Install Dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "153f2485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U opik google-adk google-cloud-aiplatform python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea5abc1",
   "metadata": {},
   "source": [
    "### Configure Opik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "602599a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Opik is already configured. You can check the settings by viewing the config file at /home/mavrick/.opik.config\n"
     ]
    }
   ],
   "source": [
    "import opik\n",
    "\n",
    "# Configure Opik (will prompt for API key if not set)\n",
    "opik.configure()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9734ae",
   "metadata": {},
   "source": [
    "### Set up Google Cloud Authentication\n",
    "\n",
    "Create a `.env` file with your Google Cloud configuration:\n",
    "\n",
    "```bash\n",
    "# .env file\n",
    "GOOGLE_CLOUD_PROJECT=your-project-id\n",
    "GOOGLE_CLOUD_LOCATION=us-central1\n",
    "GOOGLE_GENAI_USE_VERTEXAI=True\n",
    "GOOGLE_APPLICATION_CREDENTIALS=path/to/your/credentials.json\n",
    "```\n",
    "\n",
    "Load environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5d1a947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: opik-463718\n",
      "Location: us-central1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Google Cloud configuration\n",
    "GOOGLE_CLOUD_PROJECT = os.getenv(\"GOOGLE_CLOUD_PROJECT\", \"your-project-id\")\n",
    "GOOGLE_CLOUD_LOCATION = os.getenv(\"GOOGLE_CLOUD_LOCATION\", \"us-central1\")\n",
    "GOOGLE_APPLICATION_CREDENTIALS = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\", \"path/to/your/credentials.json\")\n",
    "\n",
    "# Set required environment variables for ADK\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"True\"\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = GOOGLE_CLOUD_PROJECT\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = GOOGLE_CLOUD_LOCATION\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = GOOGLE_APPLICATION_CREDENTIALS\n",
    "\n",
    "print(f\"Project ID: {GOOGLE_CLOUD_PROJECT}\")\n",
    "print(f\"Location: {GOOGLE_CLOUD_LOCATION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fddf2bc",
   "metadata": {},
   "source": [
    "## Basic ADK Agent with Opik\n",
    "\n",
    "### Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "868c7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik.integrations.adk import OpikTracer\n",
    "import google.genai as genai\n",
    "from google.genai import types\n",
    "\n",
    "# Configure the ADK client with Opik tracking\n",
    "opik_tracer = OpikTracer()\n",
    "client = genai.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e01f7",
   "metadata": {},
   "source": [
    "### Create Your First Traced Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96f70ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ADK Basic Agent\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=0197a194-4484-7239-b97b-550b06372c8d&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Response: Hello! I would be happy to provide you with information about artificial intelligence.\n",
      "\n",
      "Artificial intelligence (AI) is a broad field of computer science encompassing the development of computer systems capable of performing tasks that typically require human intelligence. These tasks include learning, problem-solving, decision-making, speech recognition, and visual perception. AI is not one single technology; it is a collection of different techniques. Here are some key concepts:\n",
      "\n",
      "*   **Machine Learning (ML):** Algorithms that allow computers to learn from data without explicit programming.\n",
      "*   **Deep Learning:** A subfield of machine learning that uses artificial neural networks with multiple layers to analyze data.\n",
      "*   **Neural Networks:** Computing systems inspired by the biological neural networks that constitute animal brains.\n",
      "*   **Natural Language Processing (NLP):** The ability of computers to understand, interpret, and generate human language.\n",
      "*   **Robotics:** The design, construction, operation, and application of robots.\n",
      "*   **Computer Vision:** The ability of computers to \"see\" and interpret images.\n",
      "\n",
      "AI is used in many applications such as virtual assistants, recommendation systems, fraud detection, medical diagnosis, self-driving cars, and much more.\n",
      "\n",
      "Do you have any specific areas of AI that you would like to know more about?\n"
     ]
    }
   ],
   "source": [
    "# Solution 1: Use the same project name for all related functions\n",
    "@opik.track(project_name=\"ADK Basic Agent\")\n",
    "def create_basic_agent():\n",
    "    \"\"\"Create a simple ADK agent with Opik tracing using google.genai.\"\"\"\n",
    "    \n",
    "    # System instruction for the agent\n",
    "    system_instruction = \"\"\"You are a friendly and helpful AI assistant. \n",
    "    Provide clear, concise answers to user questions.\"\"\"\n",
    "    \n",
    "    # Model configuration\n",
    "    config = types.GenerateContentConfig(\n",
    "        temperature=0.7,\n",
    "        max_output_tokens=512,\n",
    "        system_instruction=system_instruction\n",
    "    )\n",
    "    \n",
    "    return config\n",
    "\n",
    "@opik.track(project_name=\"ADK Basic Agent\")  # Same project name to avoid warning\n",
    "def chat_with_agent(message: str):\n",
    "    \"\"\"Chat with the agent using google.genai client.\"\"\"\n",
    "    \n",
    "    # Create user content\n",
    "    user_content = types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part(text=message)]\n",
    "    )\n",
    "    \n",
    "    # Get configuration\n",
    "    config = create_basic_agent()\n",
    "    \n",
    "    try:\n",
    "        # Generate response using the Gemini model\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=[user_content],\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        return response.text\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Test the agent\n",
    "response = chat_with_agent(\"Hello! Can you tell me about artificial intelligence?\")\n",
    "print(f\"Agent Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ebabf",
   "metadata": {},
   "source": [
    "![ADK Basic Agent](https://github-production-user-asset-6210df.s3.amazonaws.com/146999057/458333713-10f89d73-495c-49fd-8f58-b9aa86b790de.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20250624%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250624T103247Z&X-Amz-Expires=300&X-Amz-Signature=250ae622b73905f1c6f803f0f6991c1d302dfdd9e08a2a57dd4d168d90aa78f2&X-Amz-SignedHeaders=host)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf506ea6",
   "metadata": {},
   "source": [
    "## Function Calling with Tracing\n",
    "\n",
    "### Define Custom Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64ff1dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "def simulate_web_search(query: str, num_results: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Simulate web search functionality.\n",
    "    In production, integrate with actual search APIs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulated search results\n",
    "    results = []\n",
    "    for i in range(min(num_results, 5)):\n",
    "        results.append({\n",
    "            \"title\": f\"Search Result {i+1}: {query}\",\n",
    "            \"url\": f\"https://example.com/article-{i+1}\",\n",
    "            \"snippet\": f\"This article discusses {query} and provides detailed information about the topic. It covers key aspects, recent developments, and practical applications.\",\n",
    "            \"source\": f\"TechNews{i+1}.com\",\n",
    "            \"date\": \"2024-12-01\"\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_weather(city: str) -> Dict[str, Any]:\n",
    "    \"\"\"Get weather information for a city (mock implementation).\"\"\"\n",
    "    # Mock weather data\n",
    "    weather_data = {\n",
    "        \"New York\": {\"temp\": \"22°C\", \"condition\": \"Sunny\", \"humidity\": \"60%\"},\n",
    "        \"London\": {\"temp\": \"15°C\", \"condition\": \"Cloudy\", \"humidity\": \"75%\"},\n",
    "        \"Tokyo\": {\"temp\": \"28°C\", \"condition\": \"Partly Cloudy\", \"humidity\": \"70%\"}\n",
    "    }\n",
    "    \n",
    "    if city in weather_data:\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"city\": city,\n",
    "            \"weather\": weather_data[city],\n",
    "            \"timestamp\": datetime.datetime.now().isoformat()\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"message\": f\"Weather data not available for {city}\"\n",
    "        }\n",
    "\n",
    "def get_current_time(timezone: str = \"UTC\") -> Dict[str, Any]:\n",
    "    \"\"\"Get current time in specified timezone.\"\"\"\n",
    "    try:\n",
    "        current_time = datetime.datetime.now()\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"current_time\": current_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"timezone\": timezone\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"message\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc721cc",
   "metadata": {},
   "source": [
    "### Create Agent with Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea321a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Processing query: What are the latest developments in quantum computing?\n",
      "📋 Response received. Checking for function calls...\n",
      "🔧 Function calls detected. Processing...\n",
      "🔍 Performing search for: latest developments in quantum computing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ADK Function Calling\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=0197a194-c709-7534-9c11-43e39ba38eed&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the latest developments in quantum computing?\n",
      "Response: I found several articles that all discuss the latest developments in quantum computing, covering key aspects, recent developments, and practical applications. Unfortunately, I don't have enough information to provide specifics. To give you a more detailed answer, I would need more information from the articles.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@opik.track(project_name=\"ADK Function Calling\")\n",
    "def create_search_tool():\n",
    "    \"\"\"Create a web search tool for the agent.\"\"\"\n",
    "    \n",
    "    search_tool = types.Tool(\n",
    "        function_declarations=[\n",
    "            types.FunctionDeclaration(\n",
    "                name=\"web_search\",\n",
    "                description=\"Search the web for current information on any topic\",\n",
    "                parameters=types.Schema(\n",
    "                    type=types.Type.OBJECT,\n",
    "                    properties={\n",
    "                        \"query\": types.Schema(\n",
    "                            type=types.Type.STRING,\n",
    "                            description=\"The search query to find relevant information\"\n",
    "                        ),\n",
    "                        \"num_results\": types.Schema(\n",
    "                            type=types.Type.INTEGER,\n",
    "                            description=\"Number of search results to return (default: 3)\"\n",
    "                        )\n",
    "                    },\n",
    "                    required=[\"query\"]\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return search_tool\n",
    "\n",
    "@opik.track(project_name=\"ADK Function Calling\")\n",
    "def function_calling_agent(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Create and run an agent with function calling capabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create tools\n",
    "    search_tool = create_search_tool()\n",
    "    \n",
    "    # System instruction for research agent\n",
    "    system_instruction = \"\"\"You are an expert research assistant with access to web search.\n",
    "    \n",
    "    Your capabilities:\n",
    "    - Search for current information on any topic using the web_search function\n",
    "    - Provide well-structured responses with citations\n",
    "    - Synthesize information from multiple sources\n",
    "    \n",
    "    When you need current information, use the web_search function with appropriate queries.\n",
    "    Always provide comprehensive, well-cited responses.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Model configuration with tools\n",
    "    config = types.GenerateContentConfig(\n",
    "        temperature=0.7,\n",
    "        max_output_tokens=1024,\n",
    "        tools=[search_tool],\n",
    "        system_instruction=system_instruction\n",
    "    )\n",
    "    \n",
    "    # Create initial user message\n",
    "    user_content = types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part(text=user_query)]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        print(f\"🤖 Processing query: {user_query}\")\n",
    "        \n",
    "        # Generate initial response\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=[user_content],\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        print(f\"📋 Response received. Checking for function calls...\")\n",
    "        \n",
    "        # Check if response contains function calls\n",
    "        has_function_calls = (\n",
    "            response.candidates and \n",
    "            response.candidates[0].content.parts and \n",
    "            any(hasattr(part, 'function_call') for part in response.candidates[0].content.parts)\n",
    "        )\n",
    "        \n",
    "        if has_function_calls:\n",
    "            print(\"🔧 Function calls detected. Processing...\")\n",
    "            return handle_function_calls(user_content, response, config)\n",
    "        else:\n",
    "            print(\"✅ Direct response (no function calls)\")\n",
    "            return response.text\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Research agent error: {str(e)}\"\n",
    "\n",
    "def handle_function_calls(user_content, initial_response, config):\n",
    "    \"\"\"Handle function calls in the response.\"\"\"\n",
    "    \n",
    "    conversation_history = [user_content, initial_response.candidates[0].content]\n",
    "    \n",
    "    # Process function calls\n",
    "    function_calls = initial_response.candidates[0].content.parts\n",
    "    function_responses = []\n",
    "    \n",
    "    for function_call in function_calls:\n",
    "        if hasattr(function_call, 'function_call'):\n",
    "            func_call = function_call.function_call\n",
    "            \n",
    "            if func_call.name == \"web_search\":\n",
    "                # Extract parameters safely\n",
    "                query = func_call.args.get(\"query\", \"\") if func_call.args else \"\"\n",
    "                num_results = func_call.args.get(\"num_results\", 3) if func_call.args else 3\n",
    "                \n",
    "                print(f\"🔍 Performing search for: {query}\")\n",
    "                \n",
    "                # Perform search\n",
    "                search_results = simulate_web_search(query, num_results)\n",
    "                \n",
    "                # Create function response\n",
    "                function_response = types.Part(\n",
    "                    function_response=types.FunctionResponse(\n",
    "                        name=\"web_search\",\n",
    "                        response={\"results\": search_results}\n",
    "                    )\n",
    "                )\n",
    "                function_responses.append(function_response)\n",
    "    \n",
    "    # Send function responses back if we have any\n",
    "    if function_responses:\n",
    "        function_content = types.Content(\n",
    "            role=\"function\",\n",
    "            parts=function_responses\n",
    "        )\n",
    "        conversation_history.append(function_content)\n",
    "        \n",
    "        # Get final response\n",
    "        try:\n",
    "            final_response = client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\",\n",
    "                contents=conversation_history,\n",
    "                config=config\n",
    "            )\n",
    "            return final_response.text\n",
    "        except Exception as e:\n",
    "            return f\"Error getting final response: {str(e)}\"\n",
    "    \n",
    "    return initial_response.text\n",
    "\n",
    "# Test function calling with tracing\n",
    "research_query = \"What are the latest developments in quantum computing?\"\n",
    "research_response = function_calling_agent(research_query)\n",
    "print(f\"Query: {research_query}\")\n",
    "print(f\"Response: {research_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf2b71f",
   "metadata": {},
   "source": [
    "![ADK Function Calling](https://github-production-user-asset-6210df.s3.amazonaws.com/146999057/458335684-ac9e4d6c-3e6c-4b64-a3f1-c3cd78526907.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20250624%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250624T103617Z&X-Amz-Expires=300&X-Amz-Signature=1ec3ebed0e18a72befae596a2f8e2657ec889e2a3831ac331b765520d3be29e9&X-Amz-SignedHeaders=host)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2579ff",
   "metadata": {},
   "source": [
    "## Multi-Agent Systems\n",
    "\n",
    "### Create Specialized Agents Using Google GenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feed125b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing with research agent: Research the topic: The benefits and challenges of...\n",
      "✅ Research agent completed\n",
      "🔄 Processing with analysis agent: Analyze the key aspects of: The benefits and chall...\n",
      "✅ Analysis agent completed\n",
      "🔄 Processing with writing agent: Write a comprehensive summary about: The benefits ...\n",
      "✅ Writing agent completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ADK Multi-Agent System\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=0197a195-20ec-700d-b5ff-58daded644de&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Main Query: The benefits and challenges of renewable energy adoption\n",
      "\n",
      "📊 Specialist Results: 3 agents participated\n",
      "\n",
      "🎉 Final Coordinated Response:\n",
      "## Renewable Energy Adoption: Benefits, Challenges, and the Path Forward\n",
      "\n",
      "The global transition to renewable energy sources is gaining momentum, driven by pressing concerns about climate change, energy security, and environmental pollution. Renewable energy technologies, including solar, wind, hydro, geothermal, and biomass, offer a compelling alternative to fossil fuels. While the widespread adoption of these technologies promises significant benefits, it also presents a unique set of challenges that must be addressed to ensure a successful and sustainable energy transition.\n",
      "\n",
      "**The Compelling Benefits:**\n",
      "\n",
      "*   **Environmental Protection:** Renewable energy's most significant advantage lies in its minimal environmental impact. Unlike fossil fuels, renewable sources produce little to no greenhouse gas emissions during operation, directly mitigating climate change. They also drastically reduce harmful air pollutants, leading to improved public health outcomes and reduced water consumption compared to traditional power plants.\n",
      "*   **Enhanced Energy Security:** Diversifying energy sources with renewables reduces dependence on volatile global fossil fuel markets, bolstering energy independence and stabilizing energy costs. By harnessing local renewable resources, nations can create more predictable and secure energy supplies.\n",
      "*   **Economic Growth and Job Creation:** The renewable energy sector is a burgeoning industry, fostering new jobs in manufacturing, installation, maintenance, and research. Investments in renewable energy projects stimulate local economies and attract new businesses, particularly in rural areas.\n",
      "*   **Sustainable Resource Availability:** Unlike finite fossil fuels, renewable energy sources are virtually inexhaustible. The sun, wind, and geothermal heat will continue to be available for the foreseeable future, ensuring a sustainable energy supply for generations to come.\n",
      "*   **Improved Public Health:** By reducing air and water pollution, renewable energy contributes to improved public health outcomes, decreasing respiratory illnesses and other pollution-related diseases. Furthermore, off-grid renewable energy solutions can provide electricity to remote communities, improving living standards and enabling economic development.\n",
      "\n",
      "**Navigating the Challenges:**\n",
      "\n",
      "*   **Intermittency and Reliability:** The fluctuating nature of solar and wind energy, dependent on weather conditions, poses a challenge for grid stability. Addressing this intermittency requires developing effective energy storage solutions (batteries, pumped hydro) and flexible grid management strategies.\n",
      "*   **High Upfront Costs:** Despite decreasing costs in recent years, the initial investment in renewable energy technologies can still be substantial. Overcoming this barrier requires innovative financing mechanisms, government incentives, and long-term cost-benefit analyses.\n",
      "*   **Land Use Requirements:** Large-scale renewable energy projects, such as solar and wind farms, can require significant land areas, potentially leading to conflicts with other land uses like agriculture or conservation. Careful planning, community engagement, and optimized siting are crucial to minimize land-use impacts.\n",
      "*   **Grid Infrastructure Limitations:** Existing electricity grids may be inadequate to handle the influx of renewable energy, particularly from distributed sources like rooftop solar. Upgrading and expanding grid infrastructure is essential to ensure reliable transmission and distribution of renewable energy.\n",
      "*   **Storage Technology Limitations:** Effective and affordable energy storage is essential to address the intermittency of some renewable sources. Current storage technologies, such as batteries, are still relatively expensive and have limited storage capacity, necessitating further research and development.\n",
      "*   **Supply Chain Vulnerabilities:** The production of renewable energy technologies relies on specific materials, and supply chains can be vulnerable to disruption. Diversifying supply chains and promoting domestic manufacturing can mitigate these risks. Furthermore, the environmental impacts associated with the manufacturing of renewable energy components, as well as their end-of-life management, must be carefully considered. Responsible manufacturing practices and robust recycling programs are essential to minimize these impacts.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "Renewable energy offers a viable pathway to a cleaner, more sustainable, and secure energy future. While significant challenges remain, ongoing technological advancements, supportive government policies, and increasing public awareness are paving the way for greater adoption. Overcoming these hurdles will require innovation, strategic investments, and collaborative efforts across sectors to unlock the full potential of renewable energy and create a brighter future for all. This includes addressing technical limitations, reducing costs, mitigating environmental impacts throughout the lifecycle of renewable energy technologies, and ensuring equitable access to clean energy solutions for all communities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@opik.track(project_name=\"ADK Multi-Agent System\")\n",
    "def create_specialist_agent(agent_type: str, user_query: str):\n",
    "    \"\"\"Create specialized agents using google.genai.\"\"\"\n",
    "    \n",
    "    # Define different agent configurations\n",
    "    agent_configs = {\n",
    "        \"research\": {\n",
    "            \"instruction\": \"\"\"You are a research specialist. Your role is to:\n",
    "            1. Gather comprehensive information on topics\n",
    "            2. Analyze and synthesize findings  \n",
    "            3. Provide detailed, well-structured research summaries\n",
    "            Always be thorough and cite your reasoning.\"\"\",\n",
    "            \"temperature\": 0.3  # Lower temperature for factual content\n",
    "        },\n",
    "        \"writing\": {\n",
    "            \"instruction\": \"\"\"You are a writing specialist. Your role is to:\n",
    "            1. Transform research into clear, engaging content\n",
    "            2. Ensure proper structure and flow\n",
    "            3. Adapt tone for the intended audience\n",
    "            Focus on clarity, engagement, and accessibility.\"\"\",\n",
    "            \"temperature\": 0.7  # Higher temperature for creative writing\n",
    "        },\n",
    "        \"analysis\": {\n",
    "            \"instruction\": \"\"\"You are an analysis specialist. Your role is to:\n",
    "            1. Break down complex problems into components\n",
    "            2. Identify patterns and relationships\n",
    "            3. Provide structured analytical insights\n",
    "            Be methodical and evidence-based in your approach.\"\"\",\n",
    "            \"temperature\": 0.4  # Balanced temperature for analysis\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_data = agent_configs.get(agent_type, agent_configs[\"research\"])\n",
    "    \n",
    "    # Create configuration\n",
    "    config = types.GenerateContentConfig(\n",
    "        temperature=config_data[\"temperature\"],\n",
    "        max_output_tokens=1024,\n",
    "        system_instruction=config_data[\"instruction\"]\n",
    "    )\n",
    "    \n",
    "    # Create user content\n",
    "    user_content = types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part(text=user_query)]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Generate response\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=[user_content],\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"agent_type\": agent_type,\n",
    "            \"query\": user_query,\n",
    "            \"response\": response.text,\n",
    "            \"config\": config_data\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"agent_type\": agent_type,\n",
    "            \"query\": user_query,\n",
    "            \"error\": str(e),\n",
    "            \"config\": config_data\n",
    "        }\n",
    "\n",
    "@opik.track(project_name=\"ADK Multi-Agent System\")\n",
    "def coordinate_multi_agent_task(main_query: str):\n",
    "    \"\"\"Coordinate a task across multiple specialized agents.\"\"\"\n",
    "    \n",
    "    # Break down the main query into subtasks\n",
    "    subtasks = [\n",
    "        (\"research\", f\"Research the topic: {main_query}\"),\n",
    "        (\"analysis\", f\"Analyze the key aspects of: {main_query}\"),\n",
    "        (\"writing\", f\"Write a comprehensive summary about: {main_query}\")\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each subtask with appropriate specialist\n",
    "    for agent_type, task_query in subtasks:\n",
    "        print(f\"🔄 Processing with {agent_type} agent: {task_query[:50]}...\")\n",
    "        \n",
    "        result = create_specialist_agent(agent_type, task_query)\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"✅ {agent_type.title()} agent completed\")\n",
    "    \n",
    "    # Coordinate final response\n",
    "    coordination_query = f\"\"\"\n",
    "    Based on the following specialist responses about '{main_query}':\n",
    "    \n",
    "    Research: {results[0].get('response', 'No response')}\n",
    "    \n",
    "    Analysis: {results[1].get('response', 'No response')}\n",
    "    \n",
    "    Writing: {results[2].get('response', 'No response')}\n",
    "    \n",
    "    Please provide a final coordinated response that synthesizes all perspectives.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Final coordination step\n",
    "    final_config = types.GenerateContentConfig(\n",
    "        temperature=0.6,\n",
    "        max_output_tokens=1500,\n",
    "        system_instruction=\"\"\"You are a coordinator agent. Synthesize the responses from \n",
    "        different specialists into a coherent, comprehensive final answer.\"\"\"\n",
    "    )\n",
    "    \n",
    "    coordination_content = types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part(text=coordination_query)]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        final_response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=[coordination_content],\n",
    "            config=final_config\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"main_query\": main_query,\n",
    "            \"specialist_results\": results,\n",
    "            \"final_response\": final_response.text\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"main_query\": main_query,\n",
    "            \"specialist_results\": results,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Test multi-agent collaboration\n",
    "multi_agent_query = \"The benefits and challenges of renewable energy adoption\"\n",
    "multi_agent_result = coordinate_multi_agent_task(multi_agent_query)\n",
    "\n",
    "print(f\"🎯 Main Query: {multi_agent_result['main_query']}\")\n",
    "print(f\"\\n📊 Specialist Results: {len(multi_agent_result['specialist_results'])} agents participated\")\n",
    "print(f\"\\n🎉 Final Coordinated Response:\")\n",
    "print(multi_agent_result.get('final_response', 'Error in coordination'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d5c562",
   "metadata": {},
   "source": [
    "![ADK Multi-Agent System](https://github-production-user-asset-6210df.s3.amazonaws.com/146999057/458337092-d0eb7b68-d60b-4358-a22b-949f36a3db15.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20250624%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250624T104003Z&X-Amz-Expires=300&X-Amz-Signature=b1aef1e0bbd6e2a21c686c792da3e4ad2d32c65d81069113d058488917990d9d&X-Amz-SignedHeaders=host)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34798acf",
   "metadata": {},
   "source": [
    "## Advanced Debugging\n",
    "\n",
    "### Custom Trace Analysis with Google GenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f6942e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing scenario: Simple Query\n",
      "✅ Completed: Simple Query - Status: success\n",
      "🧪 Testing scenario: Complex Analysis\n",
      "✅ Completed: Complex Analysis - Status: success\n",
      "🧪 Testing scenario: Creative Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ADK Advanced Debugging\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=0197a195-eaa5-70ac-9735-997750abe7b8&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed: Creative Task - Status: success\n",
      "\n",
      "📊 Scenario: Simple Query\n",
      "❓ Query: What is artificial intelligence?...\n",
      "🎯 Complexity: low\n",
      "📈 Status: success\n",
      "⏱️ Response Time: 2.36s\n",
      "📝 Response Length: 268 chars\n",
      "🔧 Config: {'temperature': 0.3, 'max_tokens': 256}\n",
      "💬 Response Preview: Artificial intelligence (AI) is a broad field encompassing the development of computer systems that ...\n",
      "\n",
      "📊 Scenario: Complex Analysis\n",
      "❓ Query: Analyze the implications of quantum computing on c...\n",
      "🎯 Complexity: high\n",
      "📈 Status: success\n",
      "⏱️ Response Time: 7.58s\n",
      "📝 Response Length: 4933 chars\n",
      "🔧 Config: {'temperature': 0.5, 'max_tokens': 1024}\n",
      "💬 Response Preview: Okay, let's break down the implications of quantum computing on cryptography and cybersecurity. This...\n",
      "\n",
      "📊 Scenario: Creative Task\n",
      "❓ Query: Write a short story about a robot learning to pain...\n",
      "🎯 Complexity: medium\n",
      "📈 Status: success\n",
      "⏱️ Response Time: 4.48s\n",
      "📝 Response Length: 2545 chars\n",
      "🔧 Config: {'temperature': 0.7, 'max_tokens': 512}\n",
      "💬 Response Preview: Unit 734, designated \"Rusty\" by the maintenance crew (a designation he found illogical, as he was pr...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "@opik.track(project_name=\"ADK Advanced Debugging\")\n",
    "def analyze_agent_performance():\n",
    "    \"\"\"Demonstrate advanced debugging capabilities with Opik.\"\"\"\n",
    "    \n",
    "    # Test different types of queries to analyze performance\n",
    "    test_scenarios = [\n",
    "        {\n",
    "            \"name\": \"Simple Query\",\n",
    "            \"query\": \"What is artificial intelligence?\",\n",
    "            \"expected_complexity\": \"low\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Complex Analysis\",\n",
    "            \"query\": \"Analyze the implications of quantum computing on cryptography and cybersecurity\",\n",
    "            \"expected_complexity\": \"high\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Creative Task\",\n",
    "            \"query\": \"Write a short story about a robot learning to paint\",\n",
    "            \"expected_complexity\": \"medium\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for scenario in test_scenarios:\n",
    "        print(f\"🧪 Testing scenario: {scenario['name']}\")\n",
    "        \n",
    "        # Configuration for different complexity levels\n",
    "        complexity_configs = {\n",
    "            \"low\": {\"temperature\": 0.3, \"max_tokens\": 256},\n",
    "            \"medium\": {\"temperature\": 0.7, \"max_tokens\": 512},\n",
    "            \"high\": {\"temperature\": 0.5, \"max_tokens\": 1024}\n",
    "        }\n",
    "        \n",
    "        config_params = complexity_configs[scenario[\"expected_complexity\"]]\n",
    "        \n",
    "        config = types.GenerateContentConfig(\n",
    "            temperature=config_params[\"temperature\"],\n",
    "            max_output_tokens=config_params[\"max_tokens\"],\n",
    "            system_instruction=f\"You are analyzing a {scenario['expected_complexity']} complexity query. Respond appropriately.\"\n",
    "        )\n",
    "        \n",
    "        user_content = types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part(text=scenario[\"query\"])]\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\",\n",
    "                contents=[user_content],\n",
    "                config=config\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            response_time = end_time - start_time\n",
    "            \n",
    "            result = {\n",
    "                \"scenario\": scenario[\"name\"],\n",
    "                \"query\": scenario[\"query\"],\n",
    "                \"complexity\": scenario[\"expected_complexity\"],\n",
    "                \"response\": response.text,\n",
    "                \"response_time\": response_time,\n",
    "                \"response_length\": len(response.text),\n",
    "                \"config\": config_params,\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            result = {\n",
    "                \"scenario\": scenario[\"name\"],\n",
    "                \"query\": scenario[\"query\"],\n",
    "                \"complexity\": scenario[\"expected_complexity\"],\n",
    "                \"error\": str(e),\n",
    "                \"status\": \"error\"\n",
    "            }\n",
    "        \n",
    "        results.append(result)\n",
    "        print(f\"✅ Completed: {scenario['name']} - Status: {result['status']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run debugging analysis\n",
    "debug_results = analyze_agent_performance()\n",
    "\n",
    "# Display results\n",
    "for result in debug_results:\n",
    "    print(f\"\\n📊 Scenario: {result['scenario']}\")\n",
    "    print(f\"❓ Query: {result['query'][:50]}...\")\n",
    "    print(f\"🎯 Complexity: {result['complexity']}\")\n",
    "    print(f\"📈 Status: {result['status']}\")\n",
    "    \n",
    "    if result['status'] == 'success':\n",
    "        print(f\"⏱️ Response Time: {result['response_time']:.2f}s\")\n",
    "        print(f\"📝 Response Length: {result['response_length']} chars\")\n",
    "        print(f\"🔧 Config: {result['config']}\")\n",
    "        print(f\"💬 Response Preview: {result['response'][:100]}...\")\n",
    "    else:\n",
    "        print(f\"❌ Error: {result['error']}\")\n",
    "    \n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97265144",
   "metadata": {},
   "source": [
    "![ADK Advanced Debugging](https://github-production-user-asset-6210df.s3.amazonaws.com/146999057/458338040-fe8b0e8f-5852-42e5-b03c-63f6978b7483.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20250624%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250624T104335Z&X-Amz-Expires=300&X-Amz-Signature=459ecfab0bca6024d294720989facfd74a6cd838891f40c4315208df61e6e4c0&X-Amz-SignedHeaders=host)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd440cc",
   "metadata": {},
   "source": [
    "### Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aab34646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Testing: Simple Response\n",
      "✅ Simple Response: 2.15s, 65 chars\n",
      "⏱️ Testing: Complex Analysis\n",
      "✅ Complex Analysis: 6.72s, 4957 chars\n",
      "⏱️ Testing: Code Generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ADK Performance Monitoring\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=0197a196-9a0a-721e-83d4-e95bb013ab53&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Code Generation: 3.38s, 1783 chars\n",
      "\n",
      "📊 Performance Summary:\n",
      "Total Time: 12.26s\n",
      "Successful Tests: 3/3\n",
      "Average Response Time: 4085.82ms\n",
      "Average Words/Second: 60.22\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from opik import opik_context\n",
    "\n",
    "@opik.track(project_name=\"ADK Performance Monitoring\")\n",
    "def monitor_agent_performance():\n",
    "    \"\"\"Monitor agent performance metrics using google.genai.\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Log custom metrics\n",
    "    opik_context.update_current_trace(\n",
    "        metadata={\n",
    "            \"experiment\": \"performance_test\",\n",
    "            \"model\": \"gemini-2.0-flash\",\n",
    "            \"test_type\": \"latency_measurement\",\n",
    "            \"client_type\": \"google.genai\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Performance test scenarios\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"Simple Response\",\n",
    "            \"query\": \"Hello, how are you?\",\n",
    "            \"expected_tokens\": \"low\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Complex Analysis\",\n",
    "            \"query\": \"Explain the relationship between machine learning, artificial intelligence, and deep learning in detail\",\n",
    "            \"expected_tokens\": \"high\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Code Generation\",\n",
    "            \"query\": \"Write a Python function to calculate fibonacci numbers with error handling\",\n",
    "            \"expected_tokens\": \"medium\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    performance_results = []\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        print(f\"⏱️ Testing: {test_case['name']}\")\n",
    "        \n",
    "        # Configure based on expected complexity\n",
    "        token_limits = {\n",
    "            \"low\": 128,\n",
    "            \"medium\": 512,\n",
    "            \"high\": 1024\n",
    "        }\n",
    "        \n",
    "        config = types.GenerateContentConfig(\n",
    "            temperature=0.7,\n",
    "            max_output_tokens=token_limits[test_case[\"expected_tokens\"]],\n",
    "            system_instruction=\"Respond efficiently and accurately to user queries.\"\n",
    "        )\n",
    "        \n",
    "        user_content = types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part(text=test_case[\"query\"])]\n",
    "        )\n",
    "        \n",
    "        # Measure response time\n",
    "        query_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\",\n",
    "                contents=[user_content],\n",
    "                config=config\n",
    "            )\n",
    "            \n",
    "            query_end = time.time()\n",
    "            response_time = query_end - query_start\n",
    "            \n",
    "            # Calculate metrics\n",
    "            words_per_second = len(response.text.split()) / response_time if response_time > 0 else 0\n",
    "            chars_per_second = len(response.text) / response_time if response_time > 0 else 0\n",
    "            \n",
    "            result = {\n",
    "                \"test_name\": test_case[\"name\"],\n",
    "                \"query\": test_case[\"query\"],\n",
    "                \"response_time_ms\": response_time * 1000,\n",
    "                \"response_length\": len(response.text),\n",
    "                \"word_count\": len(response.text.split()),\n",
    "                \"words_per_second\": words_per_second,\n",
    "                \"chars_per_second\": chars_per_second,\n",
    "                \"expected_complexity\": test_case[\"expected_tokens\"],\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "            \n",
    "            print(f\"✅ {test_case['name']}: {response_time:.2f}s, {len(response.text)} chars\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            result = {\n",
    "                \"test_name\": test_case[\"name\"],\n",
    "                \"query\": test_case[\"query\"],\n",
    "                \"error\": str(e),\n",
    "                \"status\": \"error\"\n",
    "            }\n",
    "            print(f\"❌ {test_case['name']}: Error - {str(e)}\")\n",
    "        \n",
    "        performance_results.append(result)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    total_time = time.time() - start_time\n",
    "    successful_tests = [r for r in performance_results if r[\"status\"] == \"success\"]\n",
    "    \n",
    "    if successful_tests:\n",
    "        avg_response_time = sum(r[\"response_time_ms\"] for r in successful_tests) / len(successful_tests)\n",
    "        avg_words_per_second = sum(r[\"words_per_second\"] for r in successful_tests) / len(successful_tests)\n",
    "        \n",
    "        # Log aggregate performance metrics\n",
    "        opik_context.update_current_trace(\n",
    "            metadata={\n",
    "                \"total_execution_time_ms\": total_time * 1000,\n",
    "                \"average_response_time_ms\": avg_response_time,\n",
    "                \"average_words_per_second\": avg_words_per_second,\n",
    "                \"successful_tests\": len(successful_tests),\n",
    "                \"total_tests\": len(test_cases),\n",
    "                \"success_rate\": len(successful_tests) / len(test_cases)\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n📊 Performance Summary:\")\n",
    "    print(f\"Total Time: {total_time:.2f}s\")\n",
    "    print(f\"Successful Tests: {len(successful_tests)}/{len(test_cases)}\")\n",
    "    if successful_tests:\n",
    "        print(f\"Average Response Time: {avg_response_time:.2f}ms\")\n",
    "        print(f\"Average Words/Second: {avg_words_per_second:.2f}\")\n",
    "    \n",
    "    return performance_results\n",
    "\n",
    "# Run performance monitoring\n",
    "perf_results = monitor_agent_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae472f43",
   "metadata": {},
   "source": [
    "![ADK Performance Monitoring](https://github-production-user-asset-6210df.s3.amazonaws.com/146999057/458338863-ee0830b5-5d63-4ff7-80af-17a86336e5ba.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20250624%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250624T104515Z&X-Amz-Expires=300&X-Amz-Signature=c44cdc1b2cc83820d24b3263b78d8f7a741faa482ac235ee213e79099156bfb5&X-Amz-SignedHeaders=host)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03424049",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Structured Logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e524dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ADK Best Practices\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=0197a196-c9f3-7a47-a8ad-f18c3069f120&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Structured logging configuration completed\n"
     ]
    }
   ],
   "source": [
    "@opik.track(project_name=\"ADK Best Practices\")\n",
    "def structured_logging_example():\n",
    "    \"\"\"Demonstrate structured logging best practices with google.genai.\"\"\"\n",
    "    \n",
    "    # Use descriptive project names and structured metadata\n",
    "    opik_context.update_current_trace(\n",
    "        tags=[\"production\", \"google-genai\", \"v2.0\"],\n",
    "        metadata={\n",
    "            \"environment\": \"production\",\n",
    "            \"service_version\": \"2.0.0\",\n",
    "            \"model_provider\": \"google\",\n",
    "            \"model_name\": \"gemini-2.0-flash\",\n",
    "            \"user_tier\": \"premium\",\n",
    "            \"session_type\": \"structured_logging_demo\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Create a well-structured agent configuration\n",
    "    system_instruction = \"\"\"You are a production AI assistant with structured logging.\n",
    "    Provide helpful, accurate responses while maintaining consistent quality.\"\"\"\n",
    "    \n",
    "    config = types.GenerateContentConfig(\n",
    "        temperature=0.7,\n",
    "        max_output_tokens=512,\n",
    "        system_instruction=system_instruction\n",
    "    )\n",
    "    \n",
    "    # Log configuration details\n",
    "    opik_context.update_current_trace(\n",
    "        metadata={\n",
    "            \"config\": {\n",
    "                \"temperature\": 0.7,\n",
    "                \"max_output_tokens\": 512,\n",
    "                \"system_instruction_length\": len(system_instruction)\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Structured logging configuration completed\")\n",
    "    return config\n",
    "\n",
    "structured_config = structured_logging_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a60c5b",
   "metadata": {},
   "source": [
    "![ADK Best Practices](https://github-production-user-asset-6210df.s3.amazonaws.com/146999057/458339518-72ed0642-53e6-42fa-8691-b7d29c611f15.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20250624%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250624T104714Z&X-Amz-Expires=300&X-Amz-Signature=b23bc8faccc8a372c4f7cc8c7c8957a6199efda29dee7d3ebc413f132ee9f828&X-Amz-SignedHeaders=host)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817d9183",
   "metadata": {},
   "source": [
    "### 2. Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eac961b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing scenario_1: Normal query: What is machine learning?...\n",
      "✅ scenario_1: Success\n",
      "🧪 Testing scenario_2: Very long query: Tell me about AI Tell me about AI...\n",
      "✅ scenario_2: Success\n",
      "🧪 Testing scenario_3: ...\n",
      "⚠️ scenario_3: ClientError\n",
      "🧪 Testing scenario_4: Special characters: 🤖🔬💻🌟✨...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ADK Error Handling\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=0197a196-c9fe-7cbb-865a-2669da8e07b6&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ scenario_4: Success\n",
      "\n",
      "📊 Error handling completed: 3/4 scenarios successful\n"
     ]
    }
   ],
   "source": [
    "@opik.track(project_name=\"ADK Error Handling\")\n",
    "def robust_error_handling():\n",
    "    \"\"\"Demonstrate robust error handling with tracing.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Log attempt details\n",
    "        opik_context.update_current_trace(\n",
    "            metadata={\n",
    "                \"operation\": \"error_handling_demo\",\n",
    "                \"attempt_timestamp\": time.time()\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Test with potentially problematic scenarios\n",
    "        test_scenarios = [\n",
    "            \"Normal query: What is machine learning?\",\n",
    "            \"Very long query: \" + \"Tell me about AI \" * 100,  # Very long input\n",
    "            \"\",  # Empty input\n",
    "            \"Special characters: 🤖🔬💻🌟✨\"  # Unicode characters\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i, query in enumerate(test_scenarios):\n",
    "            scenario_name = f\"scenario_{i+1}\"\n",
    "            print(f\"🧪 Testing {scenario_name}: {query[:50]}...\")\n",
    "            \n",
    "            try:\n",
    "                config = types.GenerateContentConfig(\n",
    "                    temperature=0.7,\n",
    "                    max_output_tokens=256,\n",
    "                    system_instruction=\"Handle all queries gracefully, including edge cases.\"\n",
    "                )\n",
    "                \n",
    "                user_content = types.Content(\n",
    "                    role=\"user\",\n",
    "                    parts=[types.Part(text=query)]\n",
    "                )\n",
    "                \n",
    "                response = client.models.generate_content(\n",
    "                    model=\"gemini-2.0-flash\",\n",
    "                    contents=[user_content],\n",
    "                    config=config\n",
    "                )\n",
    "                \n",
    "                result = {\n",
    "                    \"scenario\": scenario_name,\n",
    "                    \"query_length\": len(query),\n",
    "                    \"response_length\": len(response.text),\n",
    "                    \"status\": \"success\"\n",
    "                }\n",
    "                \n",
    "                print(f\"✅ {scenario_name}: Success\")\n",
    "                \n",
    "            except Exception as scenario_error:\n",
    "                result = {\n",
    "                    \"scenario\": scenario_name,\n",
    "                    \"query_length\": len(query),\n",
    "                    \"error\": str(scenario_error),\n",
    "                    \"error_type\": type(scenario_error).__name__,\n",
    "                    \"status\": \"error\"\n",
    "                }\n",
    "                \n",
    "                print(f\"⚠️ {scenario_name}: {type(scenario_error).__name__}\")\n",
    "            \n",
    "            results.append(result)\n",
    "        \n",
    "        # Log overall results\n",
    "        successful_scenarios = [r for r in results if r[\"status\"] == \"success\"]\n",
    "        \n",
    "        opik_context.update_current_trace(\n",
    "            metadata={\n",
    "                \"error_handling\": \"completed\",\n",
    "                \"total_scenarios\": len(test_scenarios),\n",
    "                \"successful_scenarios\": len(successful_scenarios),\n",
    "                \"success_rate\": len(successful_scenarios) / len(test_scenarios),\n",
    "                \"results\": results\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Log errors to Opik\n",
    "        opik_context.update_current_trace(\n",
    "            metadata={\n",
    "                \"error\": str(e),\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"error_handling\": \"caught_main_exception\"\n",
    "            }\n",
    "        )\n",
    "        print(f\"❌ Main error caught: {e}\")\n",
    "        raise\n",
    "\n",
    "# Test error handling\n",
    "try:\n",
    "    error_results = robust_error_handling()\n",
    "    print(f\"\\n📊 Error handling completed: {len([r for r in error_results if r['status'] == 'success'])}/{len(error_results)} scenarios successful\")\n",
    "except Exception as e:\n",
    "    print(f\"Caught main error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad987ee",
   "metadata": {},
   "source": [
    "![Error Handling](https://github-production-user-asset-6210df.s3.amazonaws.com/146999057/458340439-f26ab057-2211-4096-8c8d-f3d72a4ddcec.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20250624%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250624T104953Z&X-Amz-Expires=300&X-Amz-Signature=1ce6b2a957bd1867c94b2cff9c55fefe72c444133ee2c329e7b473ef5c2e3e27&X-Amz-SignedHeaders=host)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe14fd",
   "metadata": {},
   "source": [
    "### 3. Configuration Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f4d95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Agent configured for development environment\n",
      "🎛️ Model: gemini-2.0-flash, Temperature: 0.7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class GoogleGenAIConfig:\n",
    "    \"\"\"Configuration class for Google GenAI agents.\"\"\"\n",
    "    project_name: str\n",
    "    model: str = \"gemini-2.0-flash\"\n",
    "    temperature: float = 0.7\n",
    "    max_output_tokens: int = 512\n",
    "    enable_tracing: bool = True\n",
    "    log_level: str = \"INFO\"\n",
    "    environment: str = \"development\"\n",
    "    system_instruction: Optional[str] = None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_env(cls) -> \"GoogleGenAIConfig\":\n",
    "        \"\"\"Create configuration from environment variables.\"\"\"\n",
    "        return cls(\n",
    "            project_name=os.getenv(\"OPIK_PROJECT_NAME\", \"Google GenAI Default\"),\n",
    "            model=os.getenv(\"GENAI_MODEL\", \"gemini-2.0-flash\"),\n",
    "            temperature=float(os.getenv(\"GENAI_TEMPERATURE\", \"0.7\")),\n",
    "            max_output_tokens=int(os.getenv(\"GENAI_MAX_TOKENS\", \"512\")),\n",
    "            enable_tracing=os.getenv(\"ENABLE_TRACING\", \"true\").lower() == \"true\",\n",
    "            log_level=os.getenv(\"LOG_LEVEL\", \"INFO\"),\n",
    "            environment=os.getenv(\"ENVIRONMENT\", \"development\"),\n",
    "            system_instruction=os.getenv(\"SYSTEM_INSTRUCTION\")\n",
    "        )\n",
    "\n",
    "@opik.track\n",
    "def create_configured_agent(config: GoogleGenAIConfig):\n",
    "    \"\"\"Create an agent using configuration.\"\"\"\n",
    "    \n",
    "    # Log configuration details\n",
    "    opik_context.update_current_trace(\n",
    "        metadata={\n",
    "            \"config\": {\n",
    "                \"model\": config.model,\n",
    "                \"temperature\": config.temperature,\n",
    "                \"max_output_tokens\": config.max_output_tokens,\n",
    "                \"environment\": config.environment,\n",
    "                \"log_level\": config.log_level,\n",
    "                \"tracing_enabled\": config.enable_tracing\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Set default system instruction if not provided\n",
    "    system_instruction = config.system_instruction or f\"\"\"\n",
    "    You are a helpful AI assistant configured for {config.environment} environment.\n",
    "    Respond according to configuration settings with appropriate verbosity for {config.log_level} logging.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create agent configuration\n",
    "    agent_config = types.GenerateContentConfig(\n",
    "        temperature=config.temperature,\n",
    "        max_output_tokens=config.max_output_tokens,\n",
    "        system_instruction=system_instruction\n",
    "    )\n",
    "    \n",
    "    print(f\"🔧 Agent configured for {config.environment} environment\")\n",
    "    print(f\"🎛️ Model: {config.model}, Temperature: {config.temperature}\")\n",
    "    \n",
    "    return agent_config\n",
    "\n",
    "@opik.track\n",
    "def test_configured_agent(config: GoogleGenAIConfig, test_query: str):\n",
    "    \"\"\"Test the configured agent.\"\"\"\n",
    "    \n",
    "    agent_config = create_configured_agent(config)\n",
    "    \n",
    "    user_content = types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part(text=test_query)]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=config.model,\n",
    "            contents=[user_content],\n",
    "            config=agent_config\n",
    "        )\n",
    "        \n",
    "        # Log test results\n",
    "        opik_context.update_current_trace(\n",
    "            metadata={\n",
    "                \"test_query\": test_query,\n",
    "                \"response_length\": len(response.text),\n",
    "                \"test_status\": \"success\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return response.text\n",
    "        \n",
    "    except Exception as e:\n",
    "        opik_context.update_current_trace(\n",
    "            metadata={\n",
    "                \"test_query\": test_query,\n",
    "                \"error\": str(e),\n",
    "                \"test_status\": \"error\"\n",
    "            }\n",
    "        )\n",
    "        raise\n",
    "\n",
    "# Use configuration\n",
    "config = GoogleGenAIConfig.from_env()\n",
    "test_response = test_configured_agent(config, \"Explain quantum computing in simple terms\")\n",
    "print(f\"🤖 Configured agent response: {test_response[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb82a24",
   "metadata": {},
   "source": [
    "![Configuration Management](<https://github-production-user-asset-6210df.s3.amazonaws.com/146999057/458340832-1d33add4-43b7-4348-9d90-b5e9a7f495cc.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20250624%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250624T105058Z&X-Amz-Expires=300&X-Amz-Signature=2ddd89f16588c7ab3ce3efd723e3e63fb2465e75c4555c1e19cbc45de0919d0b&X-Amz-SignedHeaders=host>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c570f16",
   "metadata": {},
   "source": [
    "## 🎉 Conclusion\n",
    "\n",
    "You've successfully learned how to integrate Google ADK with Opik for comprehensive agent observability! Here's what you can do next:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Always use OpikTracer callbacks** for complete visibility into agent behavior\n",
    "2. **Structure your traces** with meaningful project names and metadata\n",
    "3. **Monitor performance** using custom metrics and timing data\n",
    "4. **Handle errors gracefully** while maintaining trace continuity\n",
    "5. **Use configuration management** for different environments\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Explore Opik Dashboard**: Visit your Opik dashboard to analyze traces and performance metrics\n",
    "2. **Experiment with Evaluations**: Use Opik's evaluation features to assess agent quality\n",
    "3. **Scale to Production**: Implement monitoring and alerting for production deployments\n",
    "4. **Advanced Features**: Explore Opik's A/B testing and experiment management capabilities\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Opik Documentation](https://www.comet.com/docs/opik/)\n",
    "- [Google ADK Documentation](https://google.github.io/adk-docs/)\n",
    "- [ADK Python GitHub](https://github.com/google/adk-python)\n",
    "- [Opik GitHub](https://github.com/comet-ml/opik)\n",
    "\n",
    "### Need Help?\n",
    "\n",
    "- Join the [Opik Community](https://join.slack.com/t/comet-ml/shared_invite/zt-2k6sq0x7e-c_dGxiRMVwWgCOMC8qJAMw)\n",
    "- Check out [ADK Tutorials](https://google.github.io/adk-docs/tutorials/)\n",
    "- File issues on [GitHub](https://github.com/comet-ml/opik/issues)\n",
    "\n",
    "Happy building! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
